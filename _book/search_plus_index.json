{"./":{"url":"./","title":"Introduction","keywords":"","body":"TreeviewCopyright © 1141134779@qq,com 2020 all right reserved, powered by aleen42 Bourbon的个人博客 关于我 学习清单 关于博客 Bourbon的个人博客 关于我 软件工程本科在读。 现居重庆。 热爱技术，思考和审视世界。 学习清单 Golang 计算机网络 操作系统 Mysql Redis Linux 微服务相关组件 设计模式 关于博客 本博客使用Gitbook开发， 用以记录学习中的收获，以及开发过程中的问题。同时希望分享给有需要的同学。 如有错误，希望联系改正。欢迎交流探讨。 如果本博客能够帮助到您，欢迎打赏。 Github：https://github.com/BourbonWang Email: 1141134779@qq.com Copyright © 1141134779@qq.com 2020 all right reserved，powered by GitbookFile Modify: 2021-02-07 23:45:10 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"golang/slice.html":{"url":"golang/slice.html","title":"详解 go slice","keywords":"","body":"TreeviewCopyright © 1141134779@qq,com 2020 all right reserved, powered by aleen42 详解 Go slice slice 的存储结构 创建，初始化 空 slice 和 nil slice 对数组进行切片 append() 扩容 copy () slice做函数参数 详解 Go slice 切片 slice 是golang的复合类型，是对数组的补充。数组的长度不可变，go 提供了 slice 作为“动态数组”使用。 slice 底层依赖于数组。且支持通过 append() 向slice中追加元素，长度不够时会动态扩展，通过再次slice切片，可以得到得到更小的slice结构，可以迭代、遍历等。 slice 的存储结构 slice的底层结构由3部分组成： pointer：指向底层数组某个元素的指针 Length：slice当前的长度。追加元素时，长度会扩展，最大扩展到Capacity Capacity：底层数组的长度。由于slice的存储依赖于底层数组，所以Capacity也表示了slice最大能扩展到的长度 以上每部分占用8字节，所以一个slice都是24字节。 因此可以知道 golang 创建slice的过程：先创建一个有特定长度和数据类型的底层数组，然后从这个底层数组中选取一部分元素，返回这些元素组成的集合，并将 slice 指向集合中的第一个元素。换句话说，slice自身维护了一个指针属性，指向它底层数组中的某些元素的集合。 a := make([]int,3,5) fmt.Println(a) //[0 0 0] fmt.Println(len(a)) // 3 fmt.Println(cap(a)) // 5 println(a) //[3/5]0xc000094030 上面建立了一个长度为3的切片，它的底层数组长度为5。通过println()可以看出slice的结构，[3/5]表示length 和 capacity， 0xc000094030 表示指向底层数组的指针。这个slice的存储示意图： |---------|----------|--------| | pointer | capacity | length | slice：指向底层数组的第0个元素，长度为3 | | 5 | 3 | |-|-------|----------|--------| \\|/ |-|-----------------| array:长度为5,初始值为0的数组 | 0 | 0 | 0 | 0 | 0 | |---|---|---|---|---| 创建，初始化 直接创建 a := []int{1, 2, 3} //创建len和cap都为3的slice，并赋值 b := []int{9: 3} //创建len和cap都为10的slice，并将index=9的元素赋值3 // [0 0 0 0 0 0 0 0 0 3] 使用make()。可以先为底层数组分配好内存，然后从这个底层数组中再额外生成一个slice并初始化。 slice := make([]int,5) // 创建一个len和cap都为5的slice slice := make([]int,3,5) // 创建一个len=3,cap=5的slice 空 slice 和 nil slice 当声明一个slice，但不做初始化的时候，这个slice就是一个nil slice。 var nil_slice []int nil slice表示它的指向底层数组的指针为nil。也因此，nil slice的长度和容量都为0。 empty slice表示长度为0，容量为0，但却有指向的底层数组，只不过暂时是长度为0的空数组。 empty_slice := make([]int,0) empty_slice := []int{} 无论是nil slice还是empty slice，都可以对它们进行操作，如append()、len()和cap()。 对数组进行切片 对数组切片即为将slice的指针指向该数组，并利用length截取数组某一部分。 numbers := [9]int{0, 1, 2, 3, 4, 5, 6, 7, 8} //len=9 cap=9 slice1 := numbers[1:4] //len=3 cap=8 [1 2 3] slice2 := numbers[4:] //len=5 cap=5 slice=[4 5 6 7 8] 可见，底层数组始终为numbers，cap为剩余可用的数组容量。由于slice的指针指向的数组下标不同，导致了各自可用的cap不同。 因此，对于底层数组容量为k的切片slice[i : j]来说，len = j - i，cap = k - i。 改变切片元素，底层数组同时改变。改变数组元素，与其关联的切片随之改变。 numbers[2] = 10 fmt.Println(slice1) // [1 10 3] slice2[3] = 10 fmt.Println(numbers) // [0 1 10 3 4 5 6 10 8] 当多个slice共享同一个底层数组时，如果修改了某个slice中的元素，实际上修改的是底层数组的值，其它与之关联的slice也会随之改变。当同一个底层数组有很多slice的时候，一切将变得混乱不堪，因为我们不可能记住谁在共享它。所以，需要一种特性，保证各个slice的底层数组互不影响，相关内容见下面的\"扩容\"。 append() 追加元素到slice末尾。len会增加。当追加元素后的slice仍然未达到cap时，append()新元素将赋值给底层数组。当达到cap时，扩容机制将创建新的底层数组。 append()也可以用来合并slice。append()最多允许两个参数，所以一次性只能合并两个slice。但可以将append()作为另一个append()的参数，从而实现多级合并。 s1 := []int{1, 2} s2 := []int{3, 4} s3 := append(s1, s2...) //len=4 cap=4 [1 2 3 4] s4 := append(append(s1, s3...), s2...) //len=8 cap=12 //slice=[1 2 1 2 3 4 3 4] 扩容 当新的len 等于cap时，继续追加元素将引发扩容机制，开辟新的底层数组，将原来的数据复制过去。旧的底层数组仍然会被旧slice引用，新slice和旧slice不再共享同一个底层数组。 扩容的容量将进行如下判断： 首先判断，如果新申请容量（cap）大于2倍的旧容量（old.cap），最终容量（newcap）就是新申请的容量（cap）。 否则判断，如果旧切片的长度小于1024，则最终容量(newcap)就是旧容量(old.cap)的两倍。 否则判断，如果旧切片长度大于等于1024，则最终容量（newcap）从旧容量（old.cap）开始循环增加原来的1/4，即旧容量的1.25倍、1.5倍、1.75倍……直到最终容量（newcap）大于等于新申请的容量(cap)。 如果最终容量计算值溢出，则最终容量就是新申请容量(cap)。 my_slice := []int{1, 2, 3, 4, 5} // 限定长度和容量，且让长度和容量相等 new_slice := my_slice[1:3:3] // len=2 cap=2 [2 3] // 扩容 app_slice := append(new_slice, 44) // len=3 cap=4 [2 3 44] 当限定了slice的长度和容量时，如果需要扩容，golang将生成新的底层数组，而不是对原来的数组进行扩展，因此此时的新元素不会改变原来底层数组的值。 这样就可以解决上面提到的问题，因为创建了新的底层数组，所以修改不同的slice，将不会互相影响。为了保证每次都是修改各自的底层数组，通常会切出仅一个长度、仅一个容量的新slice，这样只要对它进行任何一次扩容，就会生成一个新的底层数组，从而让每个slice的底层数组都独立。 copy () copy(dst, src) 可以将src slice拷贝到dst slice。src比dst长，就截断; src比dst短，则只拷贝src那部分。这里的长度指len而不是容量cap。 copy的返回值是拷贝成功的元素数量，所以也就是src slice或dst slice中最小的那个长度。 s1 := []int{3, 4, 5} s2 := make([]int, 2, 7) copy(s1, s2) //s2: len=2 cap=7 [3 4] slice做函数参数 前面说过，slice的数据结构类似于[3/5]0xc000094030，仍可以将slice看作一种指针。这个特性直接体现在函数参数传值上。 Go中函数的参数是按值传递的，所以调用函数时会复制一个参数的副本传递给函数。如果传递给函数的是slice，它将复制该slice副本给函数，这个副本仍然是[3/5]0xc42003df10，它仍然指向源slice的底层数组。 如果函数内部对slice进行了修改，有可能会直接影响函数外部的底层数组，从而影响其它slice。但并不总是如此，例如函数内部对slice进行扩容，扩容时生成了一个新的底层数组，函数后续的代码只对新的底层数组操作，这样就不会影响原始的底层数组。 package main import \"fmt\" func main() { s1 := make([]int, 3, 4) // [0 0 0] foo(s1) printSlice(s1) // len=3 cap=4 [10 10 10] } func foo(s []int) { for i, _ := range s { //修改slice的值，将改变底层数组 s[i] += 10 } s = append(s, 3) //扩容将创建新的底层数组 s = append(s, 4) // len=5 cap=8 [10 10 10 3 4] s[1] = 20 //此时不会改变底层数组 printSlice(s) // len=5 cap=8 [10 20 10 3 4] } func printSlice(x []int) { fmt.Printf(\"len=%d cap=%d slice=%v\\n\", len(x), cap(x), x) } Copyright © 1141134779@qq.com 2020 all right reserved，powered by GitbookFile Modify: 2021-01-31 22:59:07 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"golang/list.html":{"url":"golang/list.html","title":"常用库 container/list","keywords":"","body":"TreeviewCopyright © 1141134779@qq,com 2020 all right reserved, powered by aleen42 container/list 包学习笔记 函数和功能 节点 链表 使用 创建链表 操作 遍历链表 数据结构实现 节点 链表 插入 移除 移动 container/list 包学习笔记 golang 的链表list的使用和具体实现。list对存储的元素并没有类型限制，比较方便。实现方式值得学习借鉴。 函数和功能 首先列出节点和链表的成员函数和功能。 节点 Next() ：前趋指针 Prev() ：后趋指针 链表 Init() ：初始化链表 Len() ：链表长度 Front() ：返回第一个节点的指针 Back() ：返回最后一个节点的指针 Remove(e) ：删除节点e，返回e的值 PushFront(value) ：将value加入链表头部，返回该节点 PushBack(value) ：将value加入链表尾部，返回该节点 InsertBefore(value, e) ：将value插入节点e之前，返回该节点 InsertAfter(value, e) ：将value插入节点e之后，返回该节点 MoveToFront(e) ：将节点e移动到链表头部 MoveToBack(e) ：将节点e移动到链表尾部 MoveBefore(e, mark) ：将节点e移动到节点mark前面 MoveAfter(e, mark) ：将节点e移动到节点mark后面 PushBackList(list) ：将另一个链表list连接到本链表尾部 PushFrontList(list) ：将另一个链表list连接到本链表头部 使用 创建链表 //使用提供的New()进行初始化 l := list.New() //使用var关键字 var l list.List 操作 l := list.New() l.PushBack(1) // 1 e := l.PushBack(2) // 1,2 l.PushFront(3) // 3,1,2 e = l.InsertBefore(4,e) // 3,1,4,2 e = l.InsertAfter(5,e) // 3,1,4,5,2 l.MoveToBack(e) // 3,1,4,2,5 l2 := list.New() l2.PushBack(7) l2.PushBack(8) l2.PushBack(9) //l2: 7,8,9 l.PushBackList(l2) //l: 3,1,4,2,5,7,8,9 l.PushFrontList(l2) //l: 7,8,9,3,1,4,2,5,7,8,9 遍历链表 for e := l.Front(); e != nil; e = e.Next() { // do something with e.Value } 数据结构实现 golang 的 list 使用双向环形链表实现。用一个root节点同时表示头节点和尾节点，第一个元素root.next，最后一个元素root.prev。root本身不存储数据。 节点 首先看节点的数据结构，list存放节点属于的链表，用来检验操作的节点是否属于该链表，避免非法传参。value存储值，可以为任何类型。 type Element struct { next, prev *Element // The list to which this element belongs. list *List // The value stored with this element. Value interface{} } 成员函数有Next()和Prev(): func (e *Element) Next() *Element { if p := e.next; e.list != nil && p != &e.list.root { return p } return nil } 链表 type List struct { root Element len int } 元素的操作依靠root节点。除了root节点外，len存储节点个数，不包括root。 初始化 func (l *List) Init() *List { l.root.next = &l.root l.root.prev = &l.root l.len = 0 return l } // New returns an initialized list. func New() *List { return new(List).Init() } Front()和Back()可以看出root节点同时作为头节点和尾节点 func (l *List) Front() *Element { if l.len == 0 { return nil } return l.root.next } func (l *List) Back() *Element { if l.len == 0 { return nil } return l.root.prev } 插入 对于插入操作，首先实现了节点到节点的插入，然后在其上封装了value到节点的插入，可用于之后的各种插入操作。 func (l *List) insert(e, at *Element) *Element { e.prev = at e.next = at.next e.prev.next = e e.next.prev = e e.list = l l.len++ return e } func (l *List) insertValue(v interface{}, at *Element) *Element { return l.insert(&Element{Value: v}, at) } 所以可以容易得到PushFront()和PushBack()的实现 func (l *List) PushFront(v interface{}) *Element { l.lazyInit() return l.insertValue(v, &l.root) } func (l *List) PushBack(v interface{}) *Element { l.lazyInit() return l.insertValue(v, l.root.prev) } 注意这里使用了懒加载，在第一次使用到的时候再进行初始化 func (l *List) lazyInit() { if l.root.next == nil { l.Init() } } InsertBefore()和InsertAfter()同理，要记得检验节点是否属于该链表 func (l *List) InsertBefore(v interface{}, mark *Element) *Element { if mark.list != l { return nil } return l.insertValue(v, mark.prev) } PushBackList()和PushFrontList()是合并链表的操作。同样依赖于基本的插入操作。对传入的链表进行遍历，插入到对应的位置。 func (l *List) PushBackList(other *List) { l.lazyInit() for i, e := other.Len(), other.Front(); i > 0; i, e = i-1, e.Next() { l.insertValue(e.Value, l.root.prev) } } func (l *List) PushFrontList(other *List) { l.lazyInit() for i, e := other.Len(), other.Back(); i > 0; i, e = i-1, e.Prev() { l.insertValue(e.Value, &l.root) } } 移除 移除操作的基本操作remove()，双链表的移除，把没用的指针设为nil func (l *List) remove(e *Element) *Element { e.prev.next = e.next e.next.prev = e.prev e.next = nil // avoid memory leaks e.prev = nil // avoid memory leaks e.list = nil l.len-- return e } 然后封装得到Remove()，同样应当检查要删除的节点是否属于该链表 func (l *List) Remove(e *Element) interface{} { if e.list == l { l.remove(e) } return e.Value } 移动 实现节点到节点的移动 func (l *List) move(e, at *Element) *Element { if e == at { return e } e.prev.next = e.next e.next.prev = e.prev e.prev = at e.next = at.next e.prev.next = e e.next.prev = e return e } 然后基于这个可以实现各种移动操作 func (l *List) MoveToFront(e *Element) { if e.list != l || l.root.next == e { return } l.move(e, &l.root) } func (l *List) MoveToBack(e *Element) { if e.list != l || l.root.prev == e { return } l.move(e, l.root.prev) } func (l *List) MoveBefore(e, mark *Element) { if e.list != l || e == mark || mark.list != l { return } l.move(e, mark.prev) } func (l *List) MoveAfter(e, mark *Element) { if e.list != l || e == mark || mark.list != l { return } l.move(e, mark) } Copyright © 1141134779@qq.com 2020 all right reserved，powered by GitbookFile Modify: 2021-01-08 14:14:44 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"golang/string.html":{"url":"golang/string.html","title":"go 字符串","keywords":"","body":"TreeviewCopyright © 1141134779@qq,com 2020 all right reserved, powered by aleen42 Go 字符串 字符串 定义字符串 拼接字符串 类型转换 检查前缀或后缀 Go 字符串 字符串 Go的字符串是一个任意字节的常量序列。Go语言中字符串的字节使用UTF-8编码表示Unicode文本，因此Go语言字符串是变宽字符序列，每一个字符都用一个或者多个字符表示，这跟其他的（C++，Java，Python 3）的字符串类型有着本质上的不同，后者为定宽字符序列。Go语言这样做不仅减少了内存和硬盘空间占用，同时也不用像其它语言那样需要对使用 UTF-8 字符集的文本进行编码和解码。 其他语言的字符串中的单个字符可以被字节索引，而Go中只有在字符串只包含7位的ASCII字符时才可以被字节索引。这并不代表Go在字符串处理能力上不足，因为Go语言支持一个字符一个字符的迭代，而且标准库中存在大量的字符串操作函数，最后我们还可以将Go语言的字符串转化为Unicode码点切片（类型为 [ ]rune），切片是支持直接索引的。 注：每一个Unicode字符都有一个唯一的叫做“码点”的标识数字。在Go语言中，一个单一的码点在内存中以 rune 的形式表示，rune表示int32类型的别名 定义字符串 字符串字面量使用双引号 \"\" 或者反引号 ` 来创建。 双引号用来创建可解析的字符串，支持转义，但不能用来引用多行； 反引号用来创建原生的字符串字面量，可能由多行组成，但不支持转义，并且可以包含除了反引号外其他所有字符。 s1 := \"Bourbon \\nBlog \\n\" s2 := `Bourbon\\n blog\\n ` fmt.Print(s1) fmt.Print(s2) 上面代码输出为： bourbon Blog Bourbon\\n blog\\n 可见，反引号定义的字符串不但保留了换行，还保留了缩进。 双引号创建可解析的字符串应用最广泛，反引号用来创建原生的字符串则多用于书写多行消息，HTML以及正则表达式。 拼接字符串 虽然Go中的字符串是不可变的，但是字符串支持 + 操作和+=操作 s1 := \"Bourbon\" s2 := \"blog\" s1 += \" \" + s2 fmt.Println(s1) // Bourbon blog 但这种方式在处理大量字符串连接的场景下将非常低效。使用 bytes.Buffer 连接字符串是一个更高效的方式，它会一次性将所有的内容连接起来转化成字符串。 var b bytes.Buffer for i := 0; i 下面比较两种拼接操作的性能差距 t := time.Now() var b bytes.Buffer for i := 0; i 可见，10000次的字符串拼接会导致数量级上的性能差距。 类型转换 在大多数语言中，可轻易地将任意数据类型转型为字符串。但在go中强制将整形转为字符串，你不会得到期望的结果。 i := 123 fmt.Println(string(i)) // { fmt.Println(strconv.Itoa(i)) // 123 string()会返回整型所对应的ASCII字符，想要正确将整型转换为字符串，应当使用strconv.Itoa() 。反之，将字符串转换为整型可以使用strconv.Atoi()。 另外还可以使用fmt.Sprintf函数将几乎所有数据类型转换为字符串，但通常应保留在这样的实例上，如正在创建的字符串包含嵌入数据，而非在期望将单个整数转换为字符串时用。 i := 123 s := fmt.Sprintf(\"the number is %d.\", i) fmt.Println(s) //the number is 123. 检查前缀或后缀 在处理字符串时，想要知道一个字符串是以一个特定的字符串开始还是以一个特定的字符串结束是非常常见的情况。可以使用strings.HasPrefix和strings.HasSuffix，它们将返回一个布尔值。 fmt.Println(strings.HasPrefix(\"something\", \"some\")) //true fmt.Println(strings.HasSuffix(\"something\", \"thing\")) //true Copyright © 1141134779@qq.com 2020 all right reserved，powered by GitbookFile Modify: 2021-01-09 16:34:57 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"golang/mutex.html":{"url":"golang/mutex.html","title":"互斥锁 / 读写锁","keywords":"","body":"TreeviewCopyright © 1141134779@qq,com 2020 all right reserved, powered by aleen42 互斥锁 Mutex 与 读写锁 RWMutex 互斥锁 Mutex 使用 状态 自旋 普通模式 / 饥饿模式 读写锁 RWMutex 原理 使用 互斥锁 Mutex 与 读写锁 RWMutex 互斥锁 在操作系统中，当多线程并发运行时，可能会访问或修改到共享的代码。比如下面这个例子，多个goroutine同时修改 n 的值： n := 0 add := func() { n = n + 1 } for i := 0; i 最终的输出很有可能不是100. 这是因为 n = n + 1在底层被解释为取值、加操作、赋值的3个操作。由于上下文切换，某个线程在执行这3个操作的过程中会被中断，另一个线程开始执行，从而导致了数据不一致。 这些共享资源的代码部分称为临界区（Critical Section）。应当保证临界区内代码的原子性。操作系统中使用信号量来保护临界区。对于上面这个例子，我们可以用二元信号量来保护临界区，即互斥锁。线程得到了资源，则对资源加锁，使用结束后释放锁。资源被加锁，其他线程将无法得到该资源，直到锁被释放。这样就保证了同时仅有1个线程正在访问资源。 Mutex 使用 Go 提供了 sync.Mutex来实现这个功能。Mutex只有两个方法：Lock() 和 Unlock()，分别代表了加锁和解锁。对于上面的例子，使用Mutex 来保护临界区： n := 0 mu := sync.Mutex{} add := func() { n = n + 1 } for i := 0; i 状态 查看源码，mutex的结构： type Mutex struct { state int32 sema uint32 } state 是状态码，包含了4项内容： Locked：表示是否上锁，上锁为1 未上锁为0 Woken：表示是否被唤醒，唤醒为1 未唤醒为0 Starving：表示是否为饥饿模式，饥饿模式为1 非饥饿模式为0 waiter：剩余的29位则为等待的goroutine数量 自旋 加锁时，如果当前Locked位为1，说明该锁当前由其他协程持有，尝试加锁的协程并不是马上转入阻塞，而是会持续的探测Locked位是否变为0，这个过程即为自旋过程。自旋时间很短（go设计为自旋4次），但如果在自旋过程中发现锁已被释放，那么协程可以立即获取锁。此时即便有协程被唤醒也无法获取锁，只能再次阻塞。 自旋的好处是，当加锁失败时不必立即转入阻塞，有一定机会获取到锁，这样可以避免协程的切换。 自旋的坏处是，如果自旋过程中获得锁，则马上执行该 goroutine。如果永远在自旋模式中，那么之前阻塞的goroutine 则很难获得锁，这样一来一些 goroutine 则会被阻塞时间过长。 普通模式 / 饥饿模式 go 对 mutex 的分配设计了两种模式。 在普通模式下，等待者以 FIFO 的顺序排队来获取锁，但被唤醒的等待者发现并没有获取到 mutex，并且还要与新到达的 goroutine 们竞争 mutex 的所有权。 在饥饿模式下，mutex 的所有权直接从对 mutex 执行解锁的 goroutine 传递给等待队列前面的等待者。新到达的 goroutine 们不要尝试去获取 mutex，即使它看起来是在解锁状态，也不要试图自旋。 读写锁 互斥锁的本质是当一个线程得到资源的时候，其他线程都不能访问。这样在资源同步，避免竞争的同时也降低了程序的并发性能。程序由原来的并行执行变成了串行执行。 其实，当我们对一个数据只做读操作的话，是不存在资源竞争的问题的。因为数据是不变的，不管多少个线程同时读取，都能得到同样的数据。所以问题不是出在读上，主要是写。要保证同时仅有一个线程在修改数据。所以真正的互斥应该是读和写、写和写之间，多个读者间没有互斥的必要。 因此，衍生出了读写锁。读写锁可以让多个读操作并发，但是对于写操作是完全互斥的。也就是说，当一个线程进行写操作的时候，其他线程既不能进行读操作，也不能进行写操作。 RWMutex 原理 操作系统中，可以使用信号量实现读写锁，也可以使用二元信号量即互斥锁实现。Go提供了读写锁sync.RWMutex，定义如下： type RWMutex struct { w Mutex // held if there are pending writers writerSem uint32 // 写锁需要等待读锁释放的信号量 readerSem uint32 // 读锁需要等待写锁释放的信号量 readerCount int32 // 读锁后面挂起了多少个写锁申请 readerWait int32 // 已释放了多少个读锁 } 可以看出RWMutex是基于Mutex实现的，在其基础上增加了读写的信号量。 读锁与读锁兼容，读锁与写锁互斥，写锁与写锁互斥，只有在锁释放后才可以继续申请互斥的锁： 可以同时申请多个读锁 有读锁时申请写锁将阻塞 只要有写锁，后续申请读锁和写锁都将阻塞 使用 提供了以下几个方法： //申请和释放写锁 func (rw *RWMutex) Lock() func (rw *RWMutex) Unlock() //申请和释放读锁 func (rw *RWMutex) RLock() func (rw *RWMutex) RUnlock() //返回一个实现Lock()和Unlock()的接口 func (rw *RWMutex) RLocker() Locker 如果不存在写锁，则Unlock()引发panic，如果不存在读锁，则RUnlock()引发panic 下面给出例子 package main import ( \"fmt\" \"sync\" \"time\" ) type data struct { N int RWM sync.RWMutex } func read(d *data, t time.Time) { d.RWM.RLock() time.Sleep(1 * time.Second) fmt.Printf(\"reader: n = %d, %s\\n\", d.N, time.Now().Sub(t).String()) d.RWM.RUnlock() } func write(d *data, t time.Time) { d.RWM.Lock() time.Sleep(3 * time.Second) d.N++ fmt.Printf(\"writer: n = %d, %s \\n\", d.N, time.Now().Sub(t).String()) d.RWM.Unlock() } func main() { t := time.Now() var d data for i := 0; i 创建了10个读线程，每个睡眠1秒；5个写线程，睡眠3秒。记录读者和写者的开始时间。输出如下： reader: n = 0, 1.000319542s reader: n = 0, 1.000435996s reader: n = 0, 1.000242171s reader: n = 0, 1.00048857s reader: n = 0, 1.000339988s reader: n = 0, 1.000224101s reader: n = 0, 1.000995623s writer: n = 1, 4.001359749s reader: n = 1, 5.001534292s reader: n = 1, 5.001578192s reader: n = 1, 5.001575223s writer: n = 2, 8.001839462s writer: n = 3, 11.00235864s writer: n = 4, 14.002692469s writer: n = 5, 17.002956352s 可以看出，由于可以同时读，多个读者间并没有发生阻塞。而写者由于锁机制，存在阻塞，延时3秒。 Copyright © 1141134779@qq.com 2020 all right reserved，powered by GitbookFile Modify: 2021-01-13 21:47:03 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"golang/channel.html":{"url":"golang/channel.html","title":"详解 go channel","keywords":"","body":"TreeviewCopyright © 1141134779@qq,com 2020 all right reserved, powered by aleen42 详解 Go channel channel 简介 channel 创建 channel 的类型 有缓存的 channel 无缓存的 channel nil channel channel 关闭 range select 使用规则 循环监听 超时处理 源码 类定义 make 实现 send 实现 recive 实现 close 实现 详解 Go channel channel 简介 Channel是Go中的一个核心类型，是 goroutine 之间通信的一种方式，可以类比成 Unix 中的进程的通信方式管道。在CSP模型中，并发执行实体对应goroutine，消息通道对应channel。channel 本身还需关联了一个类型，也就是 channel 可以发送数据的类型。例如: 发送 int 类型消息的 channel 写作 chan int 。 channel 创建 channel使用内置函数 make() 创建，其中包括了数据类型，以及容量capacity 。容量代表channel容纳的最多的元素的数量，代表channel的缓存的大小。如果没有设置容量，或者容量设置为0, 则创建的是无缓存channel。 ch := make(chan int, 100) channel和 slice，map 类似，make 创建了一个底层数据结构的引用，当赋值或参数传递时，只是拷贝了一个 channel 引用，指向相同的 channel 对象。和其他引用类型一样，channel 的空值为 nil 。使用 == 可以对类型相同的 channel 进行比较，只有指向相同对象或同为 nil 时，才返回 true. channel 的类型 有缓存的 channel 创建时指定了channel的容量，则为有缓存的channel。 类似一个阻塞队列（环形数组实现），当缓存未满时，向 channel 中发送消息时不会阻塞; 当缓存满时，发送操作将被阻塞，直到有其他 goroutine 从中读取消息； 相应的，当 channel 中消息不为空时，读取消息不会出现阻塞；当 channel 为空时，读取操作会造成阻塞，直到有 goroutine 向 channel 中写入消息。 通过 len 函数可以获得 chan 中的元素个数，通过 cap 函数可以得到 channel 的缓存长度。 无缓存的 channel 创建时未指定channel的容量，则为无缓存的channel。 从无缓存的 channel 中读取消息会阻塞，直到有 goroutine 向该 channel 中发送消息； 向无缓存的 channel 中发送消息也会阻塞，直到有 goroutine 从 channel 中读取消息。 func sum(s []int, c chan int) { sum := 0 for _, v := range s { sum += v } c 这个例子中，主线程x, y := 会一直被阻塞，直到有数据被发送到channel。从而实现了goroutine的同步。 nil channel 当未为channel分配内存时，channel就是nil channel。nil channel会永远阻塞对该channel的读、写操作。 var ch chan int // nil channel 因此，channel 一定要初始化后才能进行读写操作，否则会永久阻塞。 channel 关闭 golang 提供了内置的 close 函数对 channel 进行关闭操作。 ch := make(chan int) close(ch) 关闭一个nil channel 会产生 panic 重复关闭同一个 channel 会产生 panic 向一个已关闭的 channel 中发送消息会产生 panic 从已关闭的 channel 读取消息不会产生 panic，且能读出 channel 中还未被读取的消息。若消息均已读出，则会读到类型的零值。 从已关闭的 channel 中读取消息永远不会阻塞，并且会返回一个为 false 的 ok-idiom，可以用它来判断 channel 是否关闭 c := make(chan int, 10) close(c) i, ok := 关闭 channel 会产生一个广播机制，所有向 channel 读取消息的 goroutine 都会收到消息 range channel 也可以使用 range 遍历，并且会一直从 channel 中读取数据，直到有 goroutine 对改 channel 执行 close 操作，循环才会结束。如果没有goroutine 对channel 进行写操作，则会发生死锁。 func main() { ch := make(chan int, 10) ch 这个例子将输出 1 2 3，然后发生死锁，因为检测到不会再对channel进行写操作。 select 使用规则 select 可以同时监听多个 channel 的消息状态。类似于switch，但是只用于通信。 select { //do something case select 可以同时监听多个 channel 的写入或读取 执行 select 时，若只有一个 case 通过(不阻塞)，则执行这个 case 块 若有多个 case 通过，则随机挑选一个 case 执行 若所有 case 均阻塞，且定义了 default 模块，则执行 default 模块。若未定义 default 模块，则 select 语句阻塞，直到有 case 被唤醒。 nil channel上的操作会一直被阻塞，如果没有default case,只有nil channel的select会一直被阻塞。 使用 break 可以跳出 select 。 循环监听 select 和 switch 一样，它只会选择一个case来处理，如果想一直处理channel，可以在外面加一个无限的for循环，直到收到某信号后退出： for { select { case c 超时处理 因为上面我们提到，如果没有case需要处理，select语句就会一直阻塞着。这时候我们可能就需要一个超时操作，用来处理超时的情况。 func main() { c1 := make(chan string, 1) go func() { time.Sleep(time.Second * 2) c1 这个例子中，2秒后往 c1 中发送一个数据，但是 select 设置为1秒超时,因此我们会打印出timeout 1。它利用的是time.After方法，它返回一个类型为的单向的channel，在指定的时间发送一个当前时间给返回的channel中，被 select 接收，从而实现定时。 源码 $GOROOT/src/runtime/chan.go 类定义 type hchan struct { qcount uint // total data in the queue dataqsiz uint // size of the circular queue buf unsafe.Pointer // points to an array of dataqsiz elements elemsize uint16 closed uint32 elemtype *_type // element type sendx uint // send index recvx uint // receive index recvq waitq // list of recv waiters sendq waitq // list of send waiters // lock protects all fields in hchan, as well as several // fields in sudogs blocked on this channel. // // Do not change another G's status while holding this lock // (in particular, do not ready a G), as this can deadlock // with stack shrinking. lock mutex } type waitq struct { first *sudog last *sudog } make 实现 func makechan(t *chantype, size int) *hchan { elem := t.elem // compiler checks this but be safe. if elem.size >= 1 maxAlign { throw(\"makechan: bad alignment\") } mem, overflow := math.MulUintptr(elem.size, uintptr(size)) if overflow || mem > maxAlloc-hchanSize || size send 实现 // entry point for c 0 { t0 = cputicks() } lock(&c.lock) if c.closed != 0 { unlock(&c.lock) panic(plainError(\"send on closed channel\")) } if sg := c.recvq.dequeue(); sg != nil { // Found a waiting receiver. We pass the value we want to send // directly to the receiver, bypassing the channel buffer (if any). send(c, sg, ep, func() { unlock(&c.lock) }, 3) return true } if c.qcount 0 { blockevent(mysg.releasetime-t0, 2) } mysg.c = nil releaseSudog(mysg) return true } recive 实现 // entry points for 0 { t0 = cputicks() } lock(&c.lock) if c.closed != 0 && c.qcount == 0 { if raceenabled { raceacquire(c.raceaddr()) } unlock(&c.lock) if ep != nil { typedmemclr(c.elemtype, ep) } return true, false } if sg := c.sendq.dequeue(); sg != nil { // Found a waiting sender. If buffer is size 0, receive value // directly from sender. Otherwise, receive from head of queue // and add sender's value to the tail of the queue (both map to // the same buffer slot because the queue is full). recv(c, sg, ep, func() { unlock(&c.lock) }, 3) return true, true } if c.qcount > 0 { // Receive directly from queue qp := chanbuf(c, c.recvx) if raceenabled { raceacquire(qp) racerelease(qp) } if ep != nil { typedmemmove(c.elemtype, ep, qp) } typedmemclr(c.elemtype, qp) c.recvx++ if c.recvx == c.dataqsiz { c.recvx = 0 } c.qcount-- unlock(&c.lock) return true, true } if !block { unlock(&c.lock) return false, false } // no sender available: block on this channel. gp := getg() mysg := acquireSudog() mysg.releasetime = 0 if t0 != 0 { mysg.releasetime = -1 } // No stack splits between assigning elem and enqueuing mysg // on gp.waiting where copystack can find it. mysg.elem = ep mysg.waitlink = nil gp.waiting = mysg mysg.g = gp mysg.isSelect = false mysg.c = c gp.param = nil c.recvq.enqueue(mysg) gopark(chanparkcommit, unsafe.Pointer(&c.lock), waitReasonChanReceive, traceEvGoBlockRecv, 2) // someone woke us up if mysg != gp.waiting { throw(\"G waiting list is corrupted\") } gp.waiting = nil gp.activeStackChans = false if mysg.releasetime > 0 { blockevent(mysg.releasetime-t0, 2) } closed := gp.param == nil gp.param = nil mysg.c = nil releaseSudog(mysg) return true, !closed } close 实现 func closechan(c *hchan) { if c == nil { panic(plainError(\"close of nil channel\")) } lock(&c.lock) if c.closed != 0 { unlock(&c.lock) panic(plainError(\"close of closed channel\")) } if raceenabled { callerpc := getcallerpc() racewritepc(c.raceaddr(), callerpc, funcPC(closechan)) racerelease(c.raceaddr()) } c.closed = 1 var glist gList // release all readers for { sg := c.recvq.dequeue() if sg == nil { break } if sg.elem != nil { typedmemclr(c.elemtype, sg.elem) sg.elem = nil } if sg.releasetime != 0 { sg.releasetime = cputicks() } gp := sg.g gp.param = nil if raceenabled { raceacquireg(gp, c.raceaddr()) } glist.push(gp) } // release all writers (they will panic) for { sg := c.sendq.dequeue() if sg == nil { break } sg.elem = nil if sg.releasetime != 0 { sg.releasetime = cputicks() } gp := sg.g gp.param = nil if raceenabled { raceacquireg(gp, c.raceaddr()) } glist.push(gp) } unlock(&c.lock) // Ready all Gs now that we've dropped the channel lock. for !glist.empty() { gp := glist.pop() gp.schedlink = 0 goready(gp, 3) } } Copyright © 1141134779@qq.com 2020 all right reserved，powered by GitbookFile Modify: 2021-01-09 20:39:40 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"golang/json.html":{"url":"golang/json.html","title":"go 读写json文件","keywords":"","body":"TreeviewCopyright © 1141134779@qq,com 2020 all right reserved, powered by aleen42 golang 读写 json 文件 将 struct 写入 json 文件 将 struct 转换成 json 将 json 写入文件 读取 json 文件转换 struct 读取json 文件 解析到结构体 golang 读写 json 文件 将 struct 写入 json 文件 定义一个结构体，注意成员名大写，否则 json 无法读取。 type Student struct { ID int Name string Scores []int } 将 struct 转换成 json tom := Student{ ID: 1, Name: \"Tom\", Scores: []int{99, 100, 80, 77}, } json, err := json.Marshal(tom) if err != nil { log.Fatal(err) } 将 json 写入文件 创建 test.json 文件，然后写入。 err = ioutil.WriteFile(\"test.json\", json, os.ModeAppend) if err != nil { log.Fatal(err) } test.json: {\"ID\":1,\"Name\":\"Tom\",\"Scores\":[99,100,80,77]} 读取 json 文件转换 struct 读取json 文件 stu := Student{} j, err := ioutil.ReadFile(\"test.json\") if err != nil { log.Fatal(err) } 解析到结构体 err = json.Unmarshal(j, &stu) if err != nil { log.Fatal(err) } fmt.Print(stu) // {1 Tom [99 100 80 77]} Copyright © 1141134779@qq.com 2020 all right reserved，powered by GitbookFile Modify: 2021-01-10 20:25:42 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"network/http.html":{"url":"network/http.html","title":"HTTP 和 HTTPS","keywords":"","body":"TreeviewCopyright © 1141134779@qq,com 2020 all right reserved, powered by aleen42 HTTP 与 HTTPS HTTP 特点 基于请求和响应 无状态 无连接 简单快速、灵活 请求报文 请求行 请求头 请求体 响应报文 状态行 响应头 存在的问题 HTTPS 加密 数字摘要 数字签名 过程 不足 安全性 成本 HTTP 与 HTTPS HTTP HTTP是超文本传输协议的缩写，是一个基于请求与响应，无状态的，应用层的协议，常基于TCP/IP协议传输数据，互联网上应用最为广泛的一种网络协议，所有的WWW文件都必须遵守这个标准。设计HTTP的初衷是为了提供一种发布和接收HTML页面的方法。随着发展，HTTP可以支持传输任何类型的数据。 特点 基于请求和响应 HTTP协议一般用于B/S架构，浏览器向服务端发送请求报文，服务器解析后，将数据包含在响应报文中，发送给浏览器。 无状态 协议对事务处理没有记忆能力，后续处理如果需要之前的信息，则必须重传。比如访问一个网站需要反复进行登录操作。对于这个特点，现有的一些解决方案比如 利用 Cookie / Session HTTP/1.1持久连接（HTTP keep-alive）。只要任意一端没有明确提出断开连接，则保持TCP连接状态，在请求首部字段中的Connection: keep-alive即为表明使用了持久连接 这样可以让用户在登陆后一段时间内不需要再次登陆。 无连接 由于无状态特点，每次请求需要通过TCP三次握手四次挥手，和服务器重新建立连接。当一个用户与服务器进行频繁的请求时，建立TCP连接将耗费大量的开销。 为此，HTTP/1.1 支持了长连接，与同一个用户的多次数据传输可以使用同一个TCP信道。但对一个信道的多个请求存在排队，不支持同时处理多个HTTP连接。 HTTP/2.0在其上实现了多路复用，对于同时的多个请求，可以同时在信道上发送和接收，大幅缩短资源请求的耗时，在请求大数量资源时效果显著。 简单快速、灵活 客户向服务器请求服务时，只需传送请求方法和路径。HTTP允许传输任意类型的数据对象。传输的类型由Content-Type加以标记。 请求报文 一个HTTP请求报文由请求行（request line）、请求头（header）、空行和请求体4个部分组成。 请求行 请求行由三部分组成：请求方法，请求URL（不包括域名），HTTP协议版本。 请求方法有 GET：获得URI指定的资源，参数放在URL后面。传输过程中可能会被浏览器、网关等缓存，参数可以直接在地址栏看到，不适合用来传递敏感数据。一些浏览器或服务端对URL的长度有限制，不适合传递大量数据。浏览器中点击的URL都是GET请求。 POST: 请求参数放在请求体中，以key1=value1&key2=value2的形式存放，不会显示在URL中。POST请求不会被缓存，可以用来传递用户信息等敏感数据。请求体没有长度限制，所以可以传递大量数据。浏览器的表单提交、文件上传等都是POST。 HEAD：和GET相似，不过服务端接收到HEAD请求时只返回响应头，不发送响应体。使用HEAD不必传输整个资源内容，就可以得到资源的信息。所以，如果只需要查看某个页面的状态时，用HEAD更高效，因为省去了传输页面内容的时间。 PUT: 向服务器发送请求存储一个资源，并用URI作为标识。PUT和POST都是向服务器发送数据，但PUT通常指定了资源的存放位置，而POST则没有，POST的数据存放位置由服务器自己决定。 DELETE: 请求服务器删除URI指定的资源 OPTIONS: 用于获取当前URL所支持的方法。若请求成功，会在HTTP头中包含一个名为“Allow”的头，值是所支持的方法，如“GET, POST”。 TRACE: 请求服务器回送收到的请求信息，主要用于测试。 CONNECT: HTTP/1.1的方法，将连接转换为一个TCP/IP管道，通常用于SSL加密服务器的链接与非加密的HTTP代理服务器的通信。 请求头 User-Agent : 产生请求的浏览器类型 Accept : 希望接受的数据类型，比如 Accept：text/xml（application/json） Accept-Encoding: 浏览器表明自己接收的编码方法 Accept-Language: 浏览器接收的语言 Accept-Charset: 接收的字符集 Content-Length：请求体的长度 Content-Type：请求体的数据类型。比如 Content-Type：text/html（application/json） Referer：提供了该请求从哪个链接跳转而来 Connection：标明Connection: keep-alive则为长连接。请求结束后，建立的TCP连接不会关闭，当客户端再次请求相同的服务器时，将继续使用这个TCP连接。有保持时间限制。 Host: 请求的主机名，从URL中提取。 If-Modified-Since: 检查资源的修改时间，和响应头的Last-Modified一起。当请求同一个资源时，将上一次请求返回的Last-Modified填入If-Modified-Since中。服务器将这个时间与真实资源的修改时间对比。如果未过期，则返回304, 使用缓存的文件。如果过期，则返回200, 将新的资源返回给浏览器。 Cache-Control: 指定缓存机制。Public表示可以被任何缓存；Private表示只缓存到私有缓存中；no-cache表示不会被缓存。 Cookie：将cookie的值发给服务器。可以用来发送session id等。 请求体 在浏览器发送HTTP请求时，GET请求往往通过URL来发送，这时无法设置请求体，只有URL，请求数据只能放在URL的querystring中；POST请求往往来自表单提交或发送文件，POST仍然可以在URL上附带一些参数，只不过表单里的数据都放在请求体中。 实际使用HTTP作为接口时，无论GET还是POST都可以使用请求体。我们通常把所有的“控制类”信息应该放在请求头中，具体的数据放在请求体里。于是服务器端在解析时，总是会先完全解析全部的请求头部。这样服务器端总是希望能够了解请求的控制信息后，就能决定这个请求进一步如何处理，是拒绝，还是处理数据，或者直接转发。比如，收到一个请求，检查请求头看到Content-Length里的数太大，或者Content-Type自己不支持，或者Accept要求的格式无法处理，就可以直接返回失败，节省了读取请求体的带宽。 响应报文 响应报文与请求报文的结构相似，也是由状态行，响应头，空行，响应体组成。 状态行 状态行包含了HTTP版本，状态码，以及状态码的描述。 状态码由3位十进制数字组成。详细请看HTTP状态码 1xx（信息）：收到请求，需要继续执行操作 101：Continue, 客户端继续请求 2xx（成功）：请求已成功接收并处理 200：OK，请求成功 201：Created，已创建 202：Accepted, 已接受 3xx（重定向）：需要采取进一步措施才能完成请求 301：Moved Permanently，永久重定向。浏览器会记录新的URI，以后直接跳转到新的URI。 302：Found，临时重定向。以后浏览器仍然使用原有URI访问服务器，可用来流量统计等。 304：Not Modified，未修改。客户端通过If-Modified-Since检查资源时，如果资源未修改，服务器返回此状态码，不会返回任何资源，客户端使用本地的缓存。 4xx（客户端错误）：请求包含错误的语法或无法完成请求 400：Bad Request，客户端请求的语法错误，服务器无法理解 401：Unauthorized，要求用户身份验证 403：Forbidden，服务器理解请求，但是拒绝执行此请求 404：Not-Found，服务器无法根据客户端的请求找到资源。通过此代码，网站设计人员可设置\"您所请求的资源无法找到\"的个性页面 405：Method Not Allowed，客户端请求中的方法被禁止 5xx（服务器错误）：服务器在处理请求时发生错误 500：Internal Server Error，服务器内部错误，无法完成请求 501：Not Implemented，服务器不支持请求的功能，无法完成请求 502：Bad Gateway，作为网关或者代理工作的服务器尝试执行请求时，从远程服务器接收到了一个无效的响应 503：Service Unavailable，由于超载或系统维护，服务器暂时的无法处理客户端的请求。 504：Gateway Time-out，充当网关或代理的服务器，未及时从远端服务器获取请求 505：HTTP Version not supported，服务器不支持请求的HTTP协议的版本，无法完成处理 响应头 Content-Type: 响应体的数据类型 Content-Length: 响应体的长度 Content-Encoding: 响应体的编码和压缩方式 Content-Language: 响应体语言 Last-Modified: 指定资源的最后修改时间 Server: 服务器的软件信息 Connection: 和请求头的Connection相同 Location: 重定向的新地址 Date: 生成报文的具体时间 Expires: 告诉浏览器在指定过期时间内使用本地缓存 Set-Cookie: 发送cookie，用来把新创建的session id返回给客户端 存在的问题 请求信息明文传输，容易被窃听截取。 数据的完整性未校验，容易被篡改。 没有验证对方身份，存在冒充危险。 HTTPS 为了解决上述HTTP存在的问题，就用到了HTTPS。HTTPS是一种通过计算机网络进行安全通信的传输协议，经由HTTP进行通信，利用SSL/TLS建立全信道，加密数据包。HTTPS使用的主要目的是提供对网站服务器的身份认证，同时保护交换数据的隐私与完整性。 加密 发送方随机生成密钥对消息进行对称加密。使用接收方的公钥加密对称密钥，仅有接收方可以解密，从而得到对称密钥解出明文。任何中间人无法得到接收方的私钥，也就无法破解消息。 数字摘要 对消息进行哈希，得到固定长度的唯一的码，不同消息的哈希码不同，将此哈希码一同发送给接收方，用于消息验证。接收方用同样的哈希函数对收到的明文进行哈希，与收到的哈希码比较，从而可以判断消息是否完整或被更改。由于哈希函数本身保证了无法从哈希码反推出明文，因此可以保证数字摘要是可靠的。 数字签名 验证消息是否真实来自发送方，也可以认为是验证数字摘要是否真实来自发送方。发送方用发送方的私钥对数字摘要进行加密。这样仅能用发送方的公钥进行解密，中间人无法进行伪造。 过程 首先客户端通过URL访问服务端443端口请求建立SSL连接。包括加密协议，版本，随机数1等等 服务端选择合适的加密协议，产生随机数2, 返回客户端 服务端随即发送CA证书，其中包含了服务端的公钥。 客户端验证证书。生成一个随机数，即预主密钥。 客户端通过随机数1,随机数2,预主密钥组合成会话密钥。用服务端的公钥进行加密 服务端收到后用私钥解密，同样的方式组装出会话密钥。 服务端用会话密钥加密一条消息发送回客户端，用来验证是否得到了正确的会话密钥。 客户端同样用会话密钥加密一条消息，用来告诉服务端消息可以正常接收。 SSL连接建立完成。 不足 安全性 HTTPS协议的加密范围也比较有限，在黑客攻击、拒绝服务攻击、服务器劫持等方面几乎起不到什么作用 SSL证书的信用链体系并不安全，特别是在某些国家可以控制CA根证书的情况下，中间人攻击一样可行 成本 SSL证书需要购买，功能越强大的证书费用越高 HTTPS协议多次握手，会使页面的加载时间延长近50%，增加耗电。比较好的方式是采用分而治之，网站主页使用HTTP协议，有关于用户信息等方面使用HTTPS。 HTTPS连接缓存不如HTTP高效，流量成本高。 SSL涉及到的安全算法会消耗 CPU 资源，对服务器资源消耗较大。 Copyright © 1141134779@qq.com 2020 all right reserved，powered by GitbookFile Modify: 2021-01-22 18:09:07 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"network/tcp.html":{"url":"network/tcp.html","title":"TCP详解","keywords":"","body":"TreeviewCopyright © 1141134779@qq,com 2020 all right reserved, powered by aleen42 TCP 详解 特点 报文 可靠数据传输 确认应答机制 滑动窗口 超时重传 快速重传 建立和结束连接 流量控制 延迟应答 拥塞控制 加性增，乘性减（AIMD） 慢启动 TCP 详解 TCP是运输层的可靠运输协议。HTTP，HTTPS，SSH，Telnet，FTP等应用层协议都是基于TCP。 特点 TCP是面向连接的。因为在一个进程可以开始向另一个进程发送数据之前，这两个进程必须先握手，即必须相互发送一些特殊的报文，以确定数据传输所需的参数。 TCP提供全双工服务。如果进程A与进程B存在TCP连接，那么应用层数据就可以在两个进程间双向流通。 TCP连接是点对点的。即单个发送方与单个接收方间的连接。TCP用（源IP地址，源端口号，目的IP地址，目的端口号）四元组来唯一标识进程。 TCP是面向字节流的。对于每个连接，TCP会建立发送缓冲区和接收缓冲区。上层应用会将数据放入缓冲区，TCP会按MSS（最大报文段长）分割成一个个报文，在合适时机发送出去。接收数据的时候，应用也是从缓冲区读取数据。因此TCP的读与写不需要完全匹配。 TCP通过报文段检验和，确认应答，超时重传，流量控制，拥塞控制等机制提供可靠传输。 报文 32位序号: 序号是该报文段首字节的编号。 32位确认号：TCP是全双工的，在发送数据的同时也在接收对方的数据。主机A填充进报文段的确认号是主机A期望从B收到的下一字节的序号。序号和确认号是保证TCP可靠传输的关键。 4位首部长度：从首部到数据部分的偏移量，一般TCP首部长度为20字节 6位标志位 URG: 标识紧急指针是否有效 ACK: 标识确认序号是否有效 PSH: 用来提示接收端应用程序立刻将数据从tcp缓冲区读走 RST: 要求重新建立连接. 我们把含有RST标识的报文称为复位报文段 SYN: 请求建立连接. 我们把含有SYN标识的报文称为同步报文段 FIN: 通知对端, 本端即将关闭. 我们把含有FIN标识的报文称为结束报文段 16位窗口：用于流量控制，指示接收方愿意接收的字节数量 16位检验和：校验和不光包含TCP首部, 也包含TCP数据部分 可靠数据传输 确认应答机制 TCP使用序号和确认号实现可靠传输。当A向B发送数据时，携带序号Seq Num为该报文段的首字节的编号；确认号Ack Num为期望对方下次发送报文的首字节编号，同时也告诉对方Ack Num之前的数据都已接收。从直观上，发送的每个报文都应当得到一个专门的ACK回应。实际上，双方往往是在互相发送各自的数据，然后各自回应对方。因此可以在发送自己的数据的时候，顺便带上回应对方的ACK。 滑动窗口 窗口大小指的是无需等待确认应答就可以继续发送数据的最大值. 发送方将维护一个滑动窗口。窗口内的报文不需要等待ACK应答，直接发送。 对于接收方：如果一个序号n的分组被正确的接收，且按序（即上次交付给上层的是序号n-1的分组），则为序号n的分组回应ACK，然后交付给上层。对于正确但无序的分组，TCP对将其暂时缓存，应答ACK仍为最近按序正确接收的分组序号。 因此，如果发送方收到分组k的应答ACK，说明k和k之前的分组都已成功接收。发送方可以将被确认过的数据从缓冲区删掉，窗口向前移动，继续发送其他的报文。 超时重传 发送的数据报文或应答的ACK报文可能在网络中丢失。当发送报文一段时间后仍未收到确认ACK，TCP将重新发送该报文，然后重置定时器。 对于连续发送的报文，如果第一个报文触发了超时重传，而在新的超时时间之前收到了第二个报文ACK，第二个报文将不会重传。 快速重传 超时重传的问题是超时周期可能较长。当一个分组丢失时，发送方可能要等待很久才重传分组，从而增加端到端的时延。事实上，发送方往往一次发送大量的分组，当有分组丢失时，会引起大量相同的冗余ACK。如果TCP发送方收到了超过3个冗余的ACK，它就认为这之后的分组发生丢失，TCP将进行快速重传。 建立和结束连接 TCP采用三次握手来创建连接，四次挥手来断开连接。详细请见另一篇文章: TCP三次握手和四次挥手 流量控制 前面说到，双方都为该连接设置了接收缓存。当接收到正确的报文后，它就将数据放入缓存。上层的应用进程会从缓存中读取数据，但没必要数据刚到达就读取，应用进程甚至可能过很长时间后才读取。如果数据读取十分缓慢，而发送方发送的数据太多太快，会导致接收缓存溢出。 为此，TCP提供了流量控制。流量控制是保证了发送速率和接收方的读速率相匹配。接收方将剩余缓存大小填入TCP报头的“窗口”字段，用于告诉发送方，自己还有多少缓存空间。窗口越大，说明接收方的吞吐量越高。由于TCP是全双工的，双方都各自维护一个接收窗口。 当接收方的缓冲区快满了，会将更小的窗口通知发送方，发送方会减慢自己的发送速度。当窗口为0时，发送方不再发送数据，但会定期的发送只有一个字节的报文段，用于探测接收方的窗口。 延迟应答 如果接收数据后立即进行ACK应答，这时返回的窗口可能比较小。实际上，接收方的处理数据速度可能很快，稍等一段时间就可以得到更大的缓冲区，返回更大的窗口。窗口越大, 网络吞吐量就越大, 传输效率就越高。TCP的目标是在保证网络不拥堵的情况下尽量提高传输效率。但延迟应答也有限制： 数量限制: 每隔N个包就应答一次 时间限制: 超过最大延迟时间就应答一次 拥塞控制 数据的丢失一般是在网络拥塞时由于路由器缓存溢出引起的。因此，分组重传是网络拥塞的征兆，但不能解决网络拥塞问题。TCP需要另一些机制在面临网络拥塞时遏制发送方。TCP拥塞控制的基本思想是，当出现丢包事件（收到3个冗余ACK）时，让发送方通过减小拥塞窗口的大小来降低发送速率。TCP的拥塞控制算法包括：1. 加性增，乘性减（AIMD）; 2. 慢启动 加性增，乘性减（AIMD） 每发生一次丢包事件，发送方就将当前的窗口大小减半。但不能降到低于一个MSS（最大报文段长）。 当收到前面数据的ACK时，就可以认为当前网络没有拥塞，可以考虑增加发送速率。这种情况下，TCP每次收到一个ACK确认后就把窗口增加，每个往返时延内拥塞窗口增加1个MSS。 总而言之，TCP感受到网络无拥塞就加性的增加发送速率，感受到网络拥塞时就乘性的减小发送速率。因而称为加性增，乘性减（Additive-Increase，Multiplicative-Decrease）算法。 慢启动 在TCP连接刚开始时，初始的拥塞窗口被设为1个MSS，而实际可用的带宽往往比这大很多。仅仅线性的增加发送速率，可能要很长时间才能达到最大的速率。因此，TCP发送方在初始阶段并不是线性的增加发送速率，而是指数级的。直到发生第一个丢包事件（或到达阈值），窗口大小减为一半，然后才会加性增。这个指数级的增加发送速率的过程称为慢启动。 每当一个报文被确认后，窗口都增加1个MSS，从而使发送速率指数增长。比如：初始时只有1个MSS，发送一个报文。收到确认后，窗口增加1个MSS，然后就可以发出两个报文。这两个报文被确认后，窗口扩大到了4个MSS。因此在慢启动阶段，每过一个RTT，窗口都增加一倍。 为了方便窗口减半和控制慢启动的窗口上限，发送方记录了窗口的阈值。初始时设定为一个较大的值。慢启动阶段达到阈值后将开始加性增。然后每次增加窗口时，阈值都会随之增加。当出现丢包或超时事件时，阈值减半，即新的窗口大小。 事实上，TCP对超时事件的反应与丢包事件（收到3个冗余ACK）有所不同。当出现超时事件，TCP将阈值减半，然后将窗口直接减为1个MSS，然后重新开始慢启动阶段，直到达到减半后的阈值。TCP对丢包事件与超时事件采取不同策略是因为，当收到3个冗余ACK，仅代表一些报文丢失，而其他一些报文能够收到，TCP会尽可能的试探网络上能利用的带宽。 Copyright © 1141134779@qq.com 2020 all right reserved，powered by GitbookFile Modify: 2021-01-22 18:07:26 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"network/three-way-handshake.html":{"url":"network/three-way-handshake.html","title":"TCP三次握手和四次挥手","keywords":"","body":"TreeviewCopyright © 1141134779@qq,com 2020 all right reserved, powered by aleen42 TCP三次握手和四次挥手 三次握手 过程 第一次握手 第二次握手 第三次握手 原因 四次挥手 第一次挥手 第二次挥手 第四次挥手 常见问题 TCP三次握手和四次挥手 三次握手 过程 第一次握手 建立连接时，客户端发送SYN报文，标志位SYN=1。 随机生成初始序列号ISN，作为序列号Seq Num = client_isn。ISN动态随机生成使得每个tcp session的字节序列号没有重叠。也为了增加安全性，为了避免被第三方猜测，从而被第三方伪造的RST报文Reset。伪造的报文要想成功，需要Seq Num 位于对方的合法接收窗口内， 而由于ISN是动态随机的，猜出对方接收窗口难度加大。 规定第一次握手不携带数据。如果第一次握手携带数据，先由服务端缓存下来等建立连接后再处理，这样会导致SYN FLOOD攻击。攻击者会用大量的携带数据的握手报文，让接收方被迫开辟大量的空间来缓存这些数据，从而耗尽内存，关闭服务。 客户端进入SYN_SENT状态，等待服务器确认。 第二次握手 服务器收到SYN报文后，如果同意连接，则发出确认报文。告知客户端：客户端发送正常，服务端接收正常。 确认报文中的标志位 ACK=1, SYN=1, 确认序号Ack Num = client_isn + 1, 同时也要为自己初始化一个序列号 Seq Num= server_isn。这个报文也不能携带数据, 但是同样要消耗一个序号。 TCP服务器进程进入了SYN-RCVD状态。 第三次握手 客户端收到确认报文后，再次向服务器确认，告知服务器：客户端接收正常，服务器发送正常。 确认报文的标志位ACK=1，序列号Seq Num = client_isn + 1, 确认序号Ack Num = server_isn + 1， 第三次握手时可以携带数据。因为伪造的第三方是无法接收到第二次握手的报文，能发出第三次握手报文的用户都是合法的。 TCP连接建立，客户端进入ESTABLISHED状态。当服务器收到客户端的确认后也进入ESTABLISHED状态，就可以正常处理携带的数据。此后双方就可以开始通信了。 原因 从建立连接的过程可以知道，三次握手是可以让双方确认彼此收发能力的最小次数。除此之外，三次握手还有其他原因： 阻止重复历史连接的初始化 The principle reason for the three-way handshake is to prevent old duplicate connection initiations from causing confusion. RFC 793指出，三次握手的主要原因是防止旧的重复连接初始化造成混乱。 比如客户端发送一个Seq Num = 50的SYN报文1，由于网络阻塞，客户端又重新发送发送了一个Seq Num = 100的SYN报文2。一段时间后，服务器先收到了SYN报文1，并且返回Ack Num = 51的确认报文。客户端收到后，检查发现这个报文并不是自己期望的Ack Num = 101的报文，所以发送了RST报文来终止连接。一段时间后，服务器收到了SYN报文2，返回确认报文Ack Num = 101。客户端收到后再次确认，建立连接成功。 所以在上面这个过程中，第三次握手让客户端可以判断当前连接是否是旧的重复连接，从而选择终止连接或成功建立连接。 如果是两次握手，客户端就不能判断是否是重复连接。因为第二次握手时，服务器已经为连接分配资源，客户端只能选择建立连接, 而这个连接是没有任何用处的。这样会导致双方资源的浪费。 同步双方初始序列号 TCP通过序列号维持可靠传输。通过序列号：接收方可以丢弃重复的包；接收方可以根据序号按序接收；可以标识发出的包中，有哪些被对方成功接收。客户端发送携带初始序列号的SYN报文，需要服务端返回ACK应答，表示客户端的初始序列号成功创建滑动窗口。反之，服务端的初始序列号同样需要客户端的确认。这样的两次来回，才能确认双方的初始序列号都可以同步。在这来回的四次中，服务端确认ACK，和服务端发送自己的初始序列号可以合并在一个报文中，因此就简化成了三次握手。 避免资源浪费 如果只有两次握手，当客户端的SYN报文在网络中阻塞时，客户端没有收到来自服务器的ACK报文，就会重新发送SYN。一段时间后，当服务器接收到了SYN报文，由于没有第三次握手，服务器不清楚客户端是否收到了自己的确认报文，所以只能对每个SYN都建立一个连接。这样会导致服务器建立多个重复冗余的连接，造成资源浪费。 四次挥手 过程 第一次挥手 客户端发送FIN报文，标志位FIN=1，序列号为Seq Num为之前对方最后传过来的数据的最后一个字节的序号+1，假设Seq Num = u. 客户端停止发送数据，进入FIN-WAIT-1状态。 第二次挥手 服务端收到FIN报文后，发出确认报文，标志位ACK=1，确认序号Ack Num = u + 1，假设自己的序列号Seq Num = v。 服务端进入CLOSE-WAIT状态。TCP服务器通知高层的应用进程，客户端向服务器的方向就释放了，这时候处于半关闭状态。即客户端已经没有数据要发送了，但是服务器若发送数据，或者有数据包还在网络中，客户端依然要接受。这个状态还要持续一段时间，也就是整个CLOSE-WAIT状态持续的时间。 客户端收到服务器的确认请求后，进入FIN-WAIT-2状态，等待服务器发送FIN报文，期间继续接收服务器发送的数据包。 第三次挥手 一段时间后，服务器发送完最后的数据包，就向客户端发送FIN报文，标志位FIN=1, Ack Num = u+1, 由于CLOSE-WAIT状态可能又发送了一些数据，假设此时Seq Num = w。服务器进入LAST-ACK状态，等待客户端最后的确认。 第四次挥手 客户端收到FIN报文后，发出确认报文，标志位ACK=1，Ack Num = w+1，Seq Num = u + 1. 客户端进入TIME-WAIT状态。TCP连接并未立即释放，需要等待 2*MSL的时间，然后释放资源，进入CLOSED状态。 服务端收到确认报文后，立即进入CLOSED状态。释放资源。因此服务端结束连接的时间比客户端稍早一些。 常见问题 为什么要挥手四次？ 每个方向都需要一个FIN和ACK，所以需要四次。在三次握手中，服务端的确认ACK和建立连接SYN合并在一个报文。但在四次挥手中，由于客户端发送FIN仅代表不再发送数据，但是还能接收数据；服务端对FIN进行回应后，可能还有数据需要发送，需要等全部发完后才能发送FIN报文来表示服务端同意关闭连接。因此，服务器确认ACK和发送FIN需要分开发送，从而比三次握手多了一次。 为什么TIME_WAIT状态等待2 * MSL 的时间？ MSL是报文最大生存时间，超过这个时间报文将被丢弃。如果服务端没有收到最后来自客户端的ACK报文，超时后服务端会重新发送FIN报文。客户端收到FIN后，会重新发送ACK报文给服务端。双方一去一回的时间为2MSL。这样，客户端可以在2 MSL 的时间内收到重传的FIN报文，然后发出ACK，重置2 MSL的计时器。超过2 MSL后，则说明不再有服务端重传的FIN报文，最后的ACK报文已被服务端接收，可以关闭连接了。 需要TIME_WAIT状态的原因 保证连接正确关闭 TIME-WAIT - represents waiting for enough time to pass to be sure the remote TCP received the acknowledgment of its connection termination request. RFC 793指出，TIME_WAIT的作用是等待足够的时间来确保服务端接收到最后的ACK。 如果没有TIME_WAIT状态，当客户端最后发送的ACK报文在网络中丢失，而客户端直接进入CLOSED状态。服务端将永远无法收到最后的ACK，那么服务端则会一直处在LAST_ACK状态。 当客户端想再次建立连接，发送SYN报文后，服务端由于一直处在LAST_ACK状态，会发送RST报文给客户端，连接建立的过程就会被终止。 上面说过，将等待时间设置为2 * MSL可以保证服务端能够正常收到最后的ACK报文。 防止新的连接收到旧连接的数据包 如果没有TIME_WAIT状态，当在关闭连接之前发送的数据包1在网络中延迟，然后连接正常被关闭。这时如果双方有相同端口的新的TCP连接建立，而恰好来自旧的连接的数据包1抵达，那么就有可能将这个来自旧连接的数据包接收，从而造成错乱。 将TIME_WAIT设置成2 * MSL，保证了两个方向的数据包都已经超时丢弃，使得旧连接的数据包在网络中自然消失，从而避免了新的连接收到旧连接的数据包的情况。 TIME_WAIT状态等待时间过长有什么危害 内存占用。TCP连接的资源迟迟无法释放。如果是服务器（服务器主动发起的断开请求），可能会导致线程池占满，处理不过来新的连接。 端口占用。一个TCP连接至少消耗一个本地端口。当处在TIME_WAIT状态的连接过多，占满了端口，将导致无法创建新的连接。 如果已经建立了连接, 但是客户端突发故障了怎么办? TCP设有一个保活计时器，客户端如果出现故障，服务器不能一直等下去浪费资源。服务器每收到一次客户端的请求后都会重新复位这个计时器，时间通常是设置为2小时，若2小时还没有收到客户端的任何数据，服务器就会发送一个探测报文段，以后每隔75分钟发送一次。若一连发送10个探测报文仍然没反应，服务器就认为客户端出了故障，接着就关闭连接。 Copyright © 1141134779@qq.com 2020 all right reserved，powered by GitbookFile Modify: 2021-01-21 23:50:35 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"network/udp.html":{"url":"network/udp.html","title":"UDP详解","keywords":"","body":"TreeviewCopyright © 1141134779@qq,com 2020 all right reserved, powered by aleen42 UDP详解 特点 报文 首部 数据 UDP和TCP的区别 UDP详解 UDP是运输层的用户数据报协议，基于UDP的应用层协议有DNS，RIP等。 特点 无需建立连接。发送方与接收方在传输数据之前不建立连接，发送方只是简单的把数据发送到网络上；接收方把收到的数据报放在队列中，供应用进程读取。UDP不会产生建立连接的时延，因此DNS采用UDP。 无连接状态。TCP为保证可靠运输需要维护连接状态，包括序列号，窗口大小，缓存等参数。UDP不需要维护连接状态，也不需要跟踪这些参数。 尽最大努力交付。应用进程只要将数据交给UDP，UDP就会立刻封装成数据报发送到网络中。吞吐量不受拥塞控制算法的调节，只受应用软件生成数据的速率、传输带宽、 源端和终端主机性能的限制。 不可靠运输。UDP只是提供了运输层最基本的功能。没有确认应答、重传、拥塞控制、缓冲窗口等保证可靠运输的手段。简单网络管理协议SNMP使用UDP作为运输层，因为在网络重压情况下，可靠的、有拥塞控制的数据传输变得难以实现。 面向报文。UDP将应用进程的报文，在添加首部后就向下交付给IP层。既不拆分，也不合并，而是保留这些报文的边界。 因此，应用程序需要选择合适的报文大小。 分组首部开销小。UDP头只有8个字节。 报文 首部 16位源端口号：UDP使用（目的IP，目的端口号）二元组来唯一标识进程。有相同的目的IP和目的端口号的报文，会被同一个套接字接收，并定向到相同的应用进程。在这个过程源端口号是没用的。但如果接收方要回复一个报文给发送方时，就可以直接将报文中的源端口号提取出来，作为目的端口号。 16位目的端口号：多路分解时用于定位目的进程。 16位长度：包括首部的UDP报文长度，以字节为单位。 16位检验和：UDP检验和提供了差错检测功能，不能进行差错恢复。一些UDP会将错误报文丢弃，另一些UDP会警告应用进程。 虽然链路层也提供了差错检测，但不能保证源与目的之间所有的链路都提供差错检测。另外，即使报文段被正确传输，在报文段存储在某路由器的内存中时，也可能出现比特差错。因此UDP提供了端到端的差错检测。 发送方的UDP对全部的16比特字求和，溢出时回卷。然后将结果取反，作为检验和。比如，下面的数据： 按16位排列 0110011001100000 0101010101010101 1000111100001100 前两行求和得到 1011101110110101 与第三行求和： 1000111100001100 + ————————————————— 溢出回卷，得到 0100101011000010 将结果取反 1011010100111101 于是就得到了检验和1011010100111101。接收方将包括检验和的4个16比特字按同样的方式求和。如果结果全是1，则分组没有差错；如果结果中出现了0，则分组出现了差错。 数据 对于DNS，数据要么包含一个查询报文，要么包含一个响应报文。对于流媒体应用，数据可以是媒体抽样。 UDP和TCP的区别 TCP面向连接，UDP是无连接的。 TCP对系统资源的要求较多，UDP较少。由于UDP是无连接的，不需要保存连接的参数。TCP要为每个对等方分配资源建立连接，而UDP不需要，UDP只用一个套接字来接收和发送报文。 TCP是面向字节流的，UDP是面向报文的。TCP的发送和接收都是将字节数据暂存到缓冲区，在合适的时间发送或交付给上层。UDP是将来自上层的数据直接打包成报文，立即发送。 TCP提供可靠传输，包括确认应答机制，重传机制，流量控制，拥塞控制等。UDP不提供，可能会丢包。当然，开发者可以在应用层实现这些机制，从而可以进行可靠通信，而无需被TCP拥塞机制约束住传输速率。 也因此，UDP的报文比TCP简单。UDP头只需要8字节，TCP头有20-60字节。 由于有基于序列号的确认应答机制，TCP能保证数据的顺序。而UDP不提供，开发者只能在应用层自己实现这个功能。 TCP对网络环境有拥塞控制机制，UDP没有。拥塞控制能保持网络的可用性，而降低传输的实时性能。在一些流媒体的场景比如视频会议、音频等，应用可以容忍一些分组的丢失，而传输的实时性则更加重要，UDP往往更适合这些应用。然而，如果网络中每个用户都传输高比特率视频，而没有任何的拥塞控制，路由器中就会有大量的分组溢出，以至于几乎没有UDP分组能成功传输到目的地。此外，路径上某处由UDP造成的大量丢包，会导致TCP发送方减小发送速率，甚至挤垮TCP会话。所以提出了另外的一些机制，如DCCP，让所有的数据源（包括UDP）执行自适应的拥塞控制。 Copyright © 1141134779@qq.com 2020 all right reserved，powered by GitbookFile Modify: 2021-01-30 20:50:30 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"network/process-from-URL-to-response.html":{"url":"network/process-from-URL-to-response.html","title":"从输入URL到获得页面请求的全过程","keywords":"","body":"TreeviewCopyright © 1141134779@qq,com 2020 all right reserved, powered by aleen42 从输入URL到获得页面请求的全过程 输入URL 域名解析 建立连接 应用层 运输层 网络层 链路层 服务器处理 浏览器处理 从输入URL到获得页面请求的全过程 输入URL 向地址栏中输入URL，浏览器会先检查URL，包括去除空格，非法字符等。 假如URL为：http://www.bourbon17.site/network/tcp.html 把URL分成协议、网络地址、资源路径三部分 协议：从该计算机获取资源的方式，有HTTP、HTTPS、FTP等，不同协议有不同的通讯内容格式，如果没有，则自动补全http:// 网络地址：指连接网络上的哪一台计算机，可以是域名、IP地址，可以包括端口号。如：www.bourbon17.site 资源路径：指从服务器上获取哪一项资源。如 /network/tcp.html 域名解析 如果URL中的网络地址直接是IP+端口号（如果省略端口号，则是HTTP默认80端口），就可以直接建立连接；如果是域名，就需要先进行域名解析，就是根据域名寻找对应的IP地址。 假如解析www.bourbon17.site，浏览器会依次进行以下查找： 从浏览器缓存里查找对应的IP，因为浏览器会缓存DNS记录一段时间 如果没找到，从系统hosts文件中查找 如果没找到，从路由器缓存查找，路由器通常有自己的DNS缓存 如果没找到，发送DNS请求到本地DNS服务器，然后从本地DNS缓存差找。本地DNS服务器通常与主机在同一个局域网中，或者相隔不过几个路由器 如果没找到，本地DNS服务器开始进行递归或迭代搜索。从根DNS服务器开始，顶级域服务器，权威DNS服务器······依次查找。比如查找www.cqu.edu.cn的IP地址：先从根DNS服务器查找，然后去.cn顶级域服务器，然后去.edu二级域名服务器，然后.cqu······这样从域名的右边向左依次搜索，最终找到了对应的IP地址。前面的DNS服务器返回的都是接下来应该查询的DNS服务器的IP地址，最终的权威服务器将返回域名对应的IP。当然，每个DNS服务器都设有缓存，如果缓存命中，就可以直接返回结果。 建立连接 应用层 将域名转换成IP地址后，就可以正常进行HTTP请求了，发送HTTP报文。 运输层 HTTP使用TCP作为运输层协议，提供端到端的可靠数据传输。 首先通过三次握手与服务器建立连接。 然后将应用层的数据读入缓存，即HTTP报文。按MSS分割成若干的TCP报文，发送到网络层。 之前的DNS使用UDP，直接将报文发送到网络层。 网络层 网络层通过IP地址找到对应的主机。 首先将运输层报文套上IP报头，包括源IP、目标IP、TTL、首部检验和等，就成了IP数据报。 然后计算机将目标IP地址与子网掩码进行与运算，来判断是否目标IP与本机是否在同一子网下。如果在同一子网，就可以直接通信；如果不在，说明目标IP在外部的网络中，就需要通过网关将数据报发送出去。 从网关转发出口后，通过路由器一步一步的跳转。路由器内部维护了路由表，包括目标地址、子网掩码、网关、 接口等几项。路由器将目标IP与表中每个条目的子网掩码进行与运算，得到的结果与对应条目的目标地址进行匹配，如果匹配就会作为候选转发目标，如果不匹配就继续尝试下个条目。如果没有能匹配的，就选择默认路由0.0.0.0。 然后路由器检查该条目的网关一项。如果网关的内容是一个IP地址，说明这时下一跳的路由器IP地址；如果网关为空，说明该条目的目标地址就是最终的目的IP了。然后将数据报转发到条目对应的接口上就可以转发出去了。 链路层对传输单元的大小有限制，一个链路层帧能承载的最大数据量叫最大传输单元MTU。当MTU比IP数据报的长度要小，就需要对IP数据报分片，分出的片也要加上IP报头，变成更小的IP数据报。由于每个链路的MTU可能不同，在MTU变小时，路由器也会将IP数据报继续分片。但路由器在收到众多分片后并不会组装，只有最终的目标设备才会组装所有的IP分片。 链路层 链路层提供了网络路径上节点间的可靠传输。链路层上的设备通过MAC地址唯一标识，即物理地址。每张网卡，路由器的每个端口都有独特的MAC地址。 同样的，在IP数据报外面套上MAC头部，包括源MAC地址、目标MAC地址等，就成了帧。发送方的MAC地址可以直接从设备读取；接收方的MAC地址需要通过路由表中查询结果的IP得到。从IP到MAC的转换使用ARP协议。ARP会先检查缓存是否记录了IP对应的MAC地址。如果缓存里没有，就在网络中广播，询问这个IP对应的MAC是谁，然后对应IP的设备会回应自己的MAC，这样本机就得到了目标IP对应的MAC了。 linux使用arp -a查看ARP缓存 链路层的帧只提供节点到节点的传输，比如从计算机到路由器，路由器到路由器。帧从源MAC地址对应的网卡发送到网络，然后到达下一跳路由器后，就会解包。路由器会检查这个帧的MAC地址是否属于自己，如果不是，就直接丢弃。解包后，MAC的任务就完成了，路由器得到了IP数据报，才能进行网络层的转发等操作。 在网络传输的过程中，源IP和目标IP始终是不会变的，一直变化的是MAC地址，因为需要MAC地址在以太网内进行两个设备之间的传输。 服务器处理 服务器最终收到了数据。从链路层依次向上分解，组装若干IP分片，再分解出运输层报文，通过端口号找到进程，存到TCP连接的缓存中。当所有TCP报文都收到后，TCP将报文内容按序组装，得到浏览器发来的HTTP报文，交付给网站的后台程序。 后台解析HTTP请求，通过资源路径找到对应的资源，或者找到对应的处理函数。然后解析请求中的参数、请求体等，做出相应的处理（查询数据库，组装HTML等）。将数据写入HTTP response，按同样的方式经过网关、路由器等返回给浏览器。 浏览器处理 然后浏览器收到了来自服务器的响应，内容就是URL对应的HTML页面。浏览器在解析HTML时，会自上而下加载，并在加载过程中进行解析渲染，生成DOM节点。在解析过程中，如果遇到请求外部资源时，如图片、外链的CSS等，会继续发送请求。请求过程是异步的，并不会影响HTML的加载。 Copyright © 1141134779@qq.com 2020 all right reserved，powered by GitbookFile Modify: 2021-01-31 22:21:31 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"db/nf.html":{"url":"db/nf.html","title":"数据库的三范式","keywords":"","body":"TreeviewCopyright © 1141134779@qq,com 2020 all right reserved, powered by aleen42 数据库的三范式 第一范式（1NF） 第二范式（2NF） 第三范式（3NF） 总结 数据库的三范式 为了建立冗余较小、结构合理的数据库，设计数据库时必须遵循一定的规则。在关系型数据库中这种规则就称为范式。 第一范式（1NF） 保证列的原子性，即列的信息不能再分解，列不能是数组或集合。关系型数据库自然满足第一范式。 此外，如果在实际场景中，列中数据可以继续拆分，也应当拆分成多个列。 比如，需求要按大致地址对用户分类，那么就可以将地址列拆分成：省、市、详细地址三列。需求要按用户的姓氏和另外的性别列来称呼‘‘xx先生/女士’’，就可以将姓名列拆分成姓氏、名字两列。 第二范式（2NF） 第二范式在第一范式的基础上，要求表中的每一行都被唯一的区分，即保证主键唯一。选择一个保证互不相同的列（比如，身份证号，学号等），或自增行号，作为主键。 如果用多个列组成联合主键，不能存在一个非主键列，只依赖与其中一个主键列。应当继续拆分表。 比如，学生选课表（学号，课程，成绩，课程学分）：其中（学号，课程）作为主键，但课程学分列则只依赖于课程列。这样就存在部分依赖的问题，是不符合第二范式的。 可以拆分成两个表：学生选课表（学号，课程，成绩），课程信息表（课程，课程学分）。这样就不存在部分依赖的问题了。 第三范式（3NF） 第三范式在第二范式的基础上，要求一个表中不能包含已在其他表中已包含的非主键列。为了满足第三范式，往往要把表拆分成多个表，需要时再join。 另外一层表述是，属性间不能存在传递关系。如果某一属性依赖于其他非主键属性，而其他非主键属性又依赖于主键，那么这个属性就是间接依赖于主键的。第三范式要求表中每个属性都与主键直接依赖，不能出现间接依赖。实际上，重复的非主键列也是一种间接依赖。 比如：图书馆中有： 借书记录表（记录号，书编号，用户号，书名，会员等级） 用户表（用户号，电话，会员等级，会员权限，过期时间） 可以看出，借书记录表中会员等级是重复列，而且也存在间接依赖：记录号->书编号->书名，记录号->用户号会员->等级。应该拆分表： 借书记录表（记录号，书编号，用户号） 书表（书编号，书名） 用户表（用户号，电话，会员等级，会员权限，过期时间） 继续观察发现，用户表中存在间接依赖：用户号->会员等级->会员权限，应该继续拆分： 借书记录表（记录号，书编号，用户号） 书表（书编号，书名） 用户表（用户号，电话，会员等级，过期时间） 会员等级表（会员等级，会员权限） 直到满足第三范式。 总结 三大范式只是一般设计数据库的基本理念，可以建立冗余较小、结构合理的数据库。没有冗余的数据库未必是最好的数据库。有时为了提高运行效率，提高读性能，就必须降低范式标准，适当保留冗余数据。具体做法是： 在概念数据模型设计时遵守第三范式，降低范式标准的工作放到物理数据模型设计时考虑。降低范式就是增加字段，减少了查询时的关联，提高查询效率，因为在数据库的操作中查询的比例要远远大于DML的比例。但是反范式化一定要适度，并且在原本已满足三范式的基础上再做调整的。 Copyright © 1141134779@qq.com 2020 all right reserved，powered by GitbookFile Modify: 2021-02-04 18:21:22 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"db/mysql-transaction.html":{"url":"db/mysql-transaction.html","title":"MySQL事务","keywords":"","body":"TreeviewCopyright © 1141134779@qq,com 2020 all right reserved, powered by aleen42 MySQL 事务 特性 事务控制 并发问题 脏读 不可重复读 幻读 事务隔离 读未提交 读已提交 可重复读 串行化 MySQL 事务 当处理操作量大、复杂度高的数据时，需要很多条SQL语句一起执行，当某条操作出现错误时，已经完成的操作又可以撤销。这些SQL语句就构成了事务。MySQL中，只有Innodb引擎才支持事务。 特性 原子性（Atomicity）：一个事务中的所有操作，要么全部完成，要么全部不完成，不会在中间某个环节结束。当执行过程中发生错误时，数据库会回滚(Rollback)到事务开始之前的状态。 一致性（Consistency）：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。比如A向B转账，不可能A扣了钱，B却没收到。 隔离性（Isolation）：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离详见下文。 持久性（Durability）：事务完成后，对数据的修改就是永久保存的，不能回滚。 事务控制 在 MySQL 命令行的默认设置下，事务都是自动提交的，即执行 SQL 语句后就会马上执行 COMMIT 操作。因此要显式地开启一个事务务须使用命令 BEGIN 或 START TRANSACTION，或者执行命令 SET AUTOCOMMIT=0，用来禁止使用当前会话的自动提交。 BEGIN 或 START TRANSACTION：显式地开启一个事务； COMMIT：提交事务，事务对数据的修改会永久保存； ROLLBACK：回滚，会结束用户的事务，并撤销正在进行的所有未提交的修改； SAVEPOINT identifier：在事务中创建一个保存点 identifier，一个事务中可以有多个 SAVEPOINT； RELEASE SAVEPOINT identifier：删除一个事务的保存点 identifier，当没有指定的保存点时，执行该语句会抛出一个异常； ROLLBACK TO identifier：把事务回滚到标记点 identifier； SET TRANSACTION：设置事务的隔离级别。 并发问题 一个事务中有很多条SQL语句，当事务并发执行时，由于这些语句被交叉执行，很可能会出现以下问题。 脏读 读取了另一事务未提交的数据。比如： 事务B对数据进行了更新操作，但还未提交。另一客户端A此时查询到了B更新后的数据。但如果B进行回滚，B的更新操作就会被撤销，数据会恢复到更新之前，那么，A读到的数据就是脏数据。 不可重复读 在一个事务过程中，读取到了另一事务已提交的数据。比如： 事务A多次读取同一数据，事务B在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时（A事务未提交），结果不一致。 幻读 比如：目前工资为5000的员工有10人，事务A读取所有工资为5000的人数为10人。此时，事务B插入一条工资也为5000的记录，并提交。这时，事务A再次读取工资为5000的员工，记录为11人。此时产生了幻读。 与不可重复读相比，不可重复读的场景往往是更新，需要行级锁；幻读更多是增加和删除，需要表级锁。 事务隔离 MySQL有4种隔离级别：读未提交、读已提交、可重复读、串行化，用以解决上面的并发问题。隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大。MySQL默认的隔离级别是可重复读。 事务隔离级别 脏读 不可重复读 幻读 读未提交（read-uncommitted） 是 是 是 读已提交（read-committed） 否 是 是 可重复读（repeatable-read） 否 否 是 串行化（serializable） 否 否 否 读未提交 所有事务都可以看到没有提交事务的数据。3种并发问题都可能会发生。 读已提交 也叫提交读。事务成功提交后才可以被查询到，所以解决了脏读的问题。但是只对事务的过程进行了隔离，没有对数据进行隔离，依然存在不可重复读和幻读的问题。 可重复读 MySQL默认的隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行，解决了不可重复读的问题。但对于插入删除操作，仍然会出现幻读。 串行化 要想解决幻读的问题，只有把事务排序使之不可能冲突，把事务的并发执行变成串行，这种隔离级别可能导致大量的超时现象和锁竞争，并发性能非常差。 Copyright © 1141134779@qq.com 2020 all right reserved，powered by GitbookFile Modify: 2021-02-04 17:12:26 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"db/mysql-index.html":{"url":"db/mysql-index.html","title":"MySQL索引","keywords":"","body":"TreeviewCopyright © 1141134779@qq,com 2020 all right reserved, powered by aleen42 MySQL 索引 普通索引 创建索引 修改表结构(添加索引) 创建表的时候直接指定 删除索引 唯一索引 创建索引 修改表结构 创建表的时候直接指定 主键索引 组合索引 全文索引 注意事项 设计 使用 索引的存储类型 B-树 B+树 索引查询 聚簇索引 哈希索引 索引的优点 索引的缺点 MySQL 索引 索引的建立对于MySQL的高效运行是很重要的，索引可以大大提高MySQL的检索速度。当对一个大表使用where子句查询一个非主键时，查询速度会很慢，因为需要遍历整个表才能查找到这个值。这时如果建立了索引，就可以像主键那样直接找到值，大幅提升查询速度。 索引分为单列索引和组合索引。 单列索引，即一个索引只包含单个列，一个表可以有多个单列索引。 组合索引，即一个索引包含多个列。 普通索引 最基本的索引，没有任何限制。 创建索引 CREATE INDEX indexName ON mytable (username(length)) 如果是CHAR，VARCHAR类型，length可以小于字段实际长度；如果是BLOB和TEXT类型，必须指定 length。 修改表结构(添加索引) ALTER table mytable ADD INDEX indexName(username(length)) 创建表的时候直接指定 CREATE TABLE mytable( ID INT NOT NULL, username VARCHAR(16) NOT NULL, INDEX [indexName] (username(10)) ); 建表时，usernname长度为 16，这里用 10。这是因为一般情况下名字的长度不会超过10，这样会加速索引查询速度，还会减少索引文件的大小，提高INSERT的更新速度。 删除索引 DROP INDEX [indexName] ON mytable; 唯一索引 它与前面的普通索引类似，区别是：索引列的值必须唯一，但允许有空值。如果是组合索引，则列值的组合必须唯一。 创建索引 CREATE UNIQUE INDEX indexName ON mytable(username(length)) 修改表结构 ALTER table mytable ADD UNIQUE [indexName] (username(length)) 创建表的时候直接指定 CREATE TABLE mytable( ID INT NOT NULL, username VARCHAR(16) NOT NULL, UNIQUE [indexName] (username(length)) ); 主键索引 主键索引也是特殊的唯一索引。主键在物理层面上只有两个用途： 惟一地标识一行。 作为一个可以被外键有效引用的对象。 主键一定是唯一性索引，但主键并不允许有空值。一个表只能有一个主键，但可以有多个唯一索引。 主键是逻辑键，而索引是物理键。一般在建表的时候指定主键，同时在物理层面创建主键索引。 CREATE TABLE mytable( ID INT NOT NULL, username VARCHAR(16) NOT NULL, PRIMARY KEY(ID) ); 与之类似的，也有外键索引。如果为某个外键字段定义了一个外键约束条件，MySQL就会定义一个内部索引来帮助自己以最有效率的方式去管理和使用外键约束条件。 组合索引 类比于主键可以包含多个列，索引也可以包含多个列。这样的索引称为组合索引。 CREATE TABLE mytable( ID INT NOT NULL, username VARCHAR(16) NOT NULL, city VARCHAR(50) NOT NULL, age INT NOT NULL ); 比如，在上面的表中建立一个组合索引： ALTER TABLE mytable ADD INDEX name_city_age (name(10),city,age); 建立这样的组合索引，其实是相当于分别建立了三组组合索引：(usernname,city,age)，(usernname,city)，(usernname)。 为什么没有 (city,age)这样的组合索引呢？这是因为MySQL组合索引“最左前缀”的结果。简单的理解就是只从最左面的开始组合, 并不是只要包含这三列的查询都会用到该组合索引。下面的几个SQL就会用到这个组合索引： SELECT * FROM mytable WHERE username=\"admin\" AND city=\"郑州\" SELECT * FROM mytable WHERE username=\"admin\" 而下面的则不会用到： SELECT * FROM mytable WHERE age=20 AND city=\"郑州\" SELECT * FROM mytable WHERE city=\"郑州\" 组合索引更像是依次检索username -> city -> age，逐层的确定出一行。因为组合索引在底层存储中保证了依次有序，非常适合上面这类查询。 全文索引 对于查询文本中的关键词，可以建立全文索引。但InnoDB内部并不支持中文、日文等，因为这些语言没有分隔符。可以使用插件辅助实现中文、日文等的全文索引。与使用like '%word%'相比，全文索引效率更高。 CREATE FULLTEXT INDEX indexName ON mytable(column); select * from mytable where match(column) against('word'); match() 函数中指定的列必须和全文索引中指定的列完全相同，否则就会报错，这是因为全文索引不会记录关键字来自哪一列。如果想要对某一列使用全文索引，请单独为该列创建全文索引。此外，innodb的全文索引只会对长度大于3小于84的词建立索引。 注意事项 设计 索引字段尽量使用数字型（简单的数据类型）。在处理查询时，字符串类型会逐个比较每个字符，而对于数字类型只需要比较一次。 尽量不要让字段的默认值为NULL。在MySQL中，含有空值的列很难进行查询优化，因为它们使得索引、索引的统计信息以及比较运算更加复杂。在设计时，应该用0、一个特殊的值或者一个空串来代替NULL。 使用前缀索引并且保证较高的索引选择性。对字符串或文本进行索引，应该指定索引的前缀长度。目的是用较短的存储空间就可以索引出更多的值。 比如，用一个CHAR(255)的列来存储用户名，通常在前20个字符内就可以把搜索范围缩小到几条数据，这样建立一个短索引不仅可以提高查询速度而且可以节省磁盘空间和I/O操作；而如果CHAR(255)的列在前10-20个字符内多数值是相同的，那就不要对整个列进行索引。 关键在于，要选择足够长的前缀来保证较高的索引选择性，同时又不能太长。索引选择性是指不重复的索引值和数据表中的记录总数的比值。索引的选择性越高，查找时就可以过滤掉更多的行。唯一索引的选择性是1. 使用唯一索引。上一点说到，唯一索引的选择性最高，索引的效果最好。 使用组合索引代替多个列索引。如果需要扩展，也尽量将原来的索引扩展成组合索引，不要新建索引。比如，表中已经有了索引a，现在要添加索引(a,b)，这时直接修改原来的索引即可。 注意重复和不使用的索引。MySQL允许在相同的列上创建多个索引，无论是有意还是无意的。大多数情况下不需要使用冗余索引。对于重复和不使用的索引：可以直接删除这些索引。因为这些索引需要占用物理空间，并且也会影响更新表的性能。 使用 如果要对文本进行搜索，使用全文索引，而不是like like不要以通配符开头。以通配符%和_ 开头作为查询时，MySQL不会使用索引。 不要在列上进行运算，也不要做函数参数。比如下面两个查询都无法使用索引： select actor_id from sakila.actor where actor_id+1=5; select ... where TO_DAYS(CURRENT_DATE) - TO_DAYS(date_col) 尽量不要使用NOT IN, <>, != 操作。可以将NOT IN用NOT EXISTS代替，将a<>0用a0代替 尽量避免在 where 子句中使用 or 来连接条件。用 or 分割开的条件， 如果 or 前的条件中的列有索引， 而后面的列中没有索引， 那么涉及到的索引都不会被用到。 组合索引的使用要遵守“最左前缀”原则。对于一个组合索引(a,b,c)：查询必须从索引左边的列开始，比如查询（a,b）。不能跳过某一列，比如查询(a,c)是不行的。不能使用索引中范围条件右边的列，比如WHERE a=123 AND b LIKE 'cde%' AND c='sdgf'只能使用(a,b)的索引，因为LIKE是范围查询。 如果列类型是字符串，那么一定记得在 where 条件中把字符常量值用引号 ' ' 引起来。否则的话即便这个列上有索引，MySQL 也不会用到的，因为MySQL 默认把输入的常量值进行转换以后才进行检索。 索引的存储类型 B-树 传统用来搜索的平衡二叉树有很多，如 AVL 树，红黑树等。这些树在一般情况下查询性能非常好，但当数据非常大的时候它们就无能为力了。原因当数据量非常大时，大部分数据只能存放在磁盘上，只有需要的数据才加载到内存中。磁盘读取时间远远超过了数据在内存中比较的时间，这时程序大部分时间会阻塞在磁盘 IO 上。我们需要尽可能的减少磁盘 IO 次数来提升性能。 平衡二叉树是通过旋转来保持平衡的，而旋转是对整棵树的操作，若部分加载到内存中则无法完成旋转操作。其次平衡二叉树的高度相对较大为 log n（底数为2），这样逻辑上很近的节点实际可能非常远，无法很好的利用磁盘预读（局部性原理），因此像AVL树，红黑树这类平衡二叉树从设计上无法迎合磁盘。 空间局部性原理：如果一个存储器的某个位置被访问，那么将它附近的位置也会被访问。 B-树是专门为外部存储器设计的，如磁盘，它对于读取和写入大块数据有良好的性能，所以一般被用在文件系统及数据库中。B-树也是平衡树，相比与平衡二叉树，B-树允许每个节点有更多的子节点。它有以下特点： 所有键值分布在整颗树中（索引值和具体data都在每个节点里）； 任何一个关键字出现且只出现在一个结点中； 搜索有可能在非叶子结点结束，最好情况O(1)就能找到数据； 在关键字全集内做一次查找, 性能逼近二分查找； B-树将整个表范围分割为多个区间，区间越多，定位数据越快越精确，节点也越大。新建节点时，直接申请页大小的空间（磁盘存储单位是按 block 分的，一般为 512 Byte。磁盘一次读取若干个 block 称为一页，具体大小和操作系统有关），计算机内存分配是按页对齐的，这样一个节点只需要一次磁盘 IO。 B+树 MySQL的innodb采用B+树作为底层数据结构。B+树是B-树的变体，也是一种多路搜索树, 它与 B-树的不同之处在于: 所有data存储在叶子节点，非叶子节点并不存储真正的 data 为所有叶子结点增加了一个指针，指向下一个叶子节点 B+树的内节点并不存储data，所以一般B+树的叶子节点和内部节点大小不同，而B树的每个节点大小是相同的，为一页。另外，B+树查询的时间复杂度固定为log n（底为分叉数），而B树的查询时间复杂度与key在树的位置有关。 B+树可以很好的利用空间局部性原理。由于B+树的叶子节点的数据都是使用链表连接起来的，而且他们在磁盘里是顺序存储的，所以当读到某个值的时候，磁盘预读原理就会提前把这些数据都读进内存，使得范围查询和排序都很快。在范围查询时，比如查询500-1000之间的节点，利用磁盘预读原理可以减少磁盘IO次数。 由于B+树的内部节点只存储索引的副本和指针，不存储data，所以在同样都是读取一页的IO中，B+树能读取到的索引值多于B树。从而减少了查询需要的IO次数。 使用B+树作为索引能让数据库的查询速度上升，而使写入数据的速度下降。因为平衡树这个结构必须一直维持在一个正确的状态，增删改数据都会改变平衡树各节点中的索引数据内容，破坏树结构。因此，在每次改变数据时，DBMS必须去重新梳理树的结构以确保它的正确，这会带来不小的性能开销，这也是索引会给查询以外的操作带来副作用的原因。 索引查询 B+树可以对，>=，BETWEEN，IN，以及不以通配符开始的LIKE使用索引。 B+树可以进行以下查询： 匹配全值：对索引中的所有列都指定具体的值。例如，上图中索引可以帮助你查找出生于1960-01-01的Cuba Allen。 匹配最左前缀：你可以利用索引查找last name为Allen的人，仅仅使用索引中的第1列。根据最左前缀原则，查询必须从最左边的列开始，并且不能跳过某一索引列。比如，不能直接查询first name，也不能查询(last name, date) 匹配列前缀：例如，你可以利用索引查找last name以J开始的人，这仅仅使用索引中的第1列。 匹配值的范围查询：可以利用索引查找last name在Allen和Barrymore之间的人，仅仅使用索引中第1列。 匹配部分精确而其它部分进行范围匹配：可以利用索引查找last name为Allen，而first name以字母K开始的人。但使用范围条件后，它后面的列将无法使用索引。 仅对索引进行查询：如果查询的列都位于索引中，则不需要再多一次I/O回读元组。索引的叶子节点中已经包含要查询的数据，那么就没有必要再回表查询了，如果索引包含满足查询的所有数据，就称为覆盖索引。比如，select date from people where last_name='Allen' and first_name='Cuda';所需要的date 已经包含在了索引中，所以不需要再从真正的数据节点里读取了。 聚簇索引 当建表时，我们给表加上了主键，这时表在磁盘上的存储结构变成了B+树，整个表的索引就是主键。这就是聚簇索引。一个表只能有一个主键，一个表也只能有一个聚簇索引，因为主键的作用就是把表的存储格式转换成树状的索引。如果不指定主键，InnoDB会用一个具有唯一且非空值的索引来代替。如果不存在这样的索引，InnoDB会定义一个隐藏的主键，然后建立聚簇索引。 一个表只能有一个聚簇索引，用来存储数据。但一个表可以有多个索引，显然不能为每个索引树都复制一份数据，于是就有了非聚簇索引（二级索引）。和聚簇索引一样，非聚簇索引也用B+树作为存储结构，也用索引列的字段作为节点值。不同的是，非聚簇索引的叶子节点存放的是数据行的主键，而不是数据行本身。 每次给表新建一个索引时，索引列中的值都会被复制出来一份，用于生成新的非聚簇索引。增删改数据时，这些的索引树也都会更新。因此， 给表添加索引，会增加表的体积， 占用磁盘存储空间。 在查询非主键索引时，数据库会先通过对应的非聚簇索引找到对应数据的主键值，然后用主键去查询聚集索引，得到数据。一共进行了2次B+树查询。 无论以任何方式查询，最终都会用主键通过聚簇索引来定位到数据，聚簇索引（主键）是通往真实数据所在的唯一路径。 然而还有另外一种特殊的方法可以不使用聚簇索引就能得到数据，即覆盖索引。建立一个索引时，索引列的数据会复制一份到索引树中。如果建立的是组合索引，树的节点中就会有多个列值。比如我们建立一个组合索引 (name, age)，树中的节点也应当包含这两个字段的值。按最左前缀原则，我们可以只对name进行查询。当查询select age from mytable where name='xxx';时，如果在索引树中通过name定位到了唯一的索引项，而我们所需要的age字段也在索引项里，那么我们就可以直接返回索引项中的age字段，而不需要再获取主键、再查聚簇索引。通过这种覆盖索引直接查找的方式，可以省略覆盖索引查找的后面两个步骤，大大的提高了查询性能。 哈希索引 MySQL中，只有Memory存储引擎显示支持hash索引。哈希索引基于哈希表实现，只有精确索引所有列的查询才有效。对于每一行数据，存储引擎都会对所有的索引列计算一个哈希码。哈希索引将所有的哈希码存储在索引中，同时在哈希表中保存指向每个数据的指针。 Hash索引有以下一些限制： 由于索引仅包含哈希码和记录指针，所以，MySQL不能通过使用索引避免读取记录，即每次使用哈希索引查询到记录指针后都要回读元组查取数据。 不能使用hash索引排序。 Hash索引不支持键的部分匹配，因为是通过整个索引值来计算hash值的。 Hash索引只支持等值比较，例如使用=，IN( )和。对于WHERE price>100并不能加速查询。 访问Hash索引的速度非常快，除非有很多哈希冲突（不同的索引列值却有相同的哈希值）。当出现哈希冲突的时候，存储引擎必须遍历链表中所有的行指针，逐行进行比较，直到找到所有符合条件的行。 如果哈希冲突很多的话，一些索引维护操作的代价也会很高。当从表中删除一行时，存储引擎要遍历对应哈希值的链表中的每一行，找到并删除对应行的引用，冲突越多，代价越大。 InnoDB引擎有一个特殊的功能叫做自适应哈希索引。当InnoDB注意到某些索引值被使用得非常频繁时，它会在内存中基于缓冲池中的B+树索引上再创建一个哈希索引，这样就使B+树索引也具有哈希索引的一些优点，比如快速的哈希查找。启用自适应哈希索引后，读和写性能可以提高2倍，对于辅助索引的连接操作，性能可以提高5倍。 索引的优点 索引大大减小了服务器需要扫描的数据量 索引可以帮助服务器避免排序和临时表 索引可以将随机IO变成顺序IO 索引对于InnoDB（对索引支持行级锁）非常重要，因为它可以让查询时锁更少的元组。在MySQL5.1和更新的版本中，InnoDB可以在服务器端过滤掉行后就释放锁。但在早期的MySQL版本中，InnoDB直到事务提交时才会解锁。对不需要的元组的加锁，会增加锁的开销，降低并发性。而索引能够减少InnoDB访问的元组数。InnoDB在二级索引上使用共享锁（读锁），但访问主键索引需要排他锁（写锁）。 索引的缺点 虽然索引大大提高了查询速度，同时却会降低增删改的速度。因为MySQL不仅要修改数据，还要修改索引文件。 建立索引会增加索引文件占用的磁盘空间。如果你在一个大表上创建了多种组合索引，索引文件的会膨胀很快。MySQL里同一个数据表里的索引总数限制为16个。 如果某个数据列包含许多重复的内容，为它建立索引就没有太大的实际效果。 对于非常小的表，大部分情况下简单的全表扫描更高效； Copyright © 1141134779@qq.com 2020 all right reserved，powered by GitbookFile Modify: 2021-02-03 16:10:58 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"db/mysql-lock.html":{"url":"db/mysql-lock.html","title":"MySQL 乐观锁与悲观锁","keywords":"","body":"TreeviewCopyright © 1141134779@qq,com 2020 all right reserved, powered by aleen42 MySQL 乐观锁与悲观锁 悲观锁 共享锁 排它锁 总结 乐观锁 总结 MySQL 乐观锁与悲观锁 乐观锁和悲观锁是两种常见的资源并发锁设计思路，不是MySQL提供的锁机制。 悲观锁 悲观锁就是对并发处理持悲观态度，总认为会发生并发冲突。进行每次操作时都要先对数据锁定，然后才能操作数据。 数据库层面的悲观锁已经通过共享锁和排它锁提供，使用时可以直接调用。在使用之前，需要关闭MySQL的自动提交 set autocommit=0 共享锁 共享锁允许多个事务查询同一数据，但会阻止其他事务获取相同数据的排它锁，即阻止其他事务修改数据。类似于读写锁中的读锁。 使用时，通过在sql语句后加上lock in share mode来对资源加共享锁。 select * from table_name where .....lock in share mode 对资源加上共享锁，会阻塞其他事务修改该资源，也会阻塞其他未声明lock in share mode的查询事务。 排它锁 排它锁要求多个事务对同一资源只能有一把锁，即阻止其他资源获得共享锁和排它锁。类似于读写锁中的写锁。 使用时，通过在sql语句后加上for update来对资源加排他锁。 select * from table_name where .....for update 使用排它锁会把数据给锁住，不过需要注意一些锁的级别。InnoDB默认使用行级锁，但是只有明确地指定索引，MySQL 才会执行行级锁；否则MySQL 将会执行表级锁。 总结 悲观锁适用于可靠的持续性连接，诸如C/S应用。 对于Web应用的HTTP连接，先天不适用 锁的使用意味着性能的损耗，在高并发、锁定持续时间长的情况下，尤其严重。 Web应用的性能瓶颈多在数据库处，使用悲观锁，进一步收紧了瓶颈 非正常中止情况下的解锁机制，设计和实现起来很麻烦，成本还很高 不够严谨的设计下，可能产生不易被发现的， 的死锁问题 乐观锁 对数据并发持乐观态度，认为数据一般不会发生冲突，只有在提交更新时，才会对冲突进行检查。 数据库本身不提供乐观锁，需要自己在业务上实现。一般实现方式是：在表中增加一列版本号，或时间戳，用来唯一标识当前数据的版本。读取数据时，将版本号一同读出；每次更新时，将版本号加1（或更新时间戳）。在提交更新时，再次读出版本号，判断当前的版本号与第一次的版本号是否相等。如果相等，说明数据没有被其他事务更改，可以更新；不相等，说明数据已经过期，则拒绝更新。 如上图所示，如果更新操作顺序执行，则数据的版本依次递增，不会产生冲突。但是如果发生有不同的业务操作对同一版本的数据进行修改，那么，先提交的事务B会把数据版本更新为2，当A在B之后提交更新时发现数据的版本已经被修改了，那么A的更新操作会失败。 总结 乐观锁是基于程序实现的，所以不存在死锁的情况，适用于读多的应用场景。 乐观锁机制避免了长事务中的数据库加锁开销，大大提升了大并发量下的系统整体性能表现。 如果经常发生冲突，上层应用不断的让用户进行重新操作，这反而降低了性能，这种情况下悲观锁就比较适用。 乐观锁机制往往基于业务系统中的数据存储逻辑，来自外部的更新操作不受我们系统的控制，因此可能会造成脏数据被更新到数据库中。可以将乐观锁在数据库存储过程中实现，对外只开放基于此存储过程的数据更新途径，而不是将数据库表直接对外公开。 Copyright © 1141134779@qq.com 2020 all right reserved，powered by GitbookFile Modify: 2021-02-06 16:57:31 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"db/mysql-log.html":{"url":"db/mysql-log.html","title":"MySQL日志","keywords":"","body":"TreeviewCopyright © 1141134779@qq,com 2020 all right reserved, powered by aleen42 MySQL日志 错误日志 一般查询日志 慢查询日志 二进制日志 相关变量 记录方式 查看 恢复数据库 MySQL日志 错误日志 错误日志会记录如下信息 mysql执行过程中的错误信息 mysql执行过程中的告警信息 event scheduler运行时所产生信息 mysq启动和停止过程中产生的信息 主从复制结构中，重从服务器IO复杂线程的启动信息 查看错误日志的位置，和告警信息 show variables where variable_name=\"log_error\" or variable_name=\"log_warnings\" +---------------+------------------------------------------------------+ | Variable_name | Value | +---------------+------------------------------------------------------+ | log_error | /var/log/mysqld.log | | log_warnings | 2 | +---------------+------------------------------------------------------+ 2 rows in set (0.001 sec) log_error是错误日志的位置，log_warnings表示是否记录告警信息到错误日志，0表示不记录告警信息，1表示记录告警信息，大于1表示各类告警信息 一般查询日志 查询日志分为一般查询日志和慢查询日志，它们是通过查询是否超出变量 long_query_time 指定时间的值来判定的。在超时时间内完成的查询是一般查询，可以将其记录到一般查询日志中；超出时间的查询是慢查询，可以将其记录到慢查询日志中。 一般查询日志记录了数据库执行的命令，不管这些语法是否正确，都会被记录。由于数据库操作命令非常多而且比较频繁，所以开启了查询日志以后，数据库可能需要不停的写入查询，这样会增大服务器的IO压力，增加很多的系统开销，所以默认情况下，mysql的一般查询日志是没有开启的。 show variables where variable_name = \"long_query_time\" or variable_name like \"%general_log%\" or variable_name = \"log_output\"; +------------------+---------------------------------------------------+ | Variable_name | Value | +------------------+---------------------------------------------------+ | general_log | OFF | | general_log_file | /usr/local/mysql/data/iZbp1ja865ota0u0aap1jmZ.log | | log_output | FILE | | long_query_time | 10.000000 | +------------------+---------------------------------------------------+ general_log表示查询日志是否开启，ON表示开启，OFF表示未开启，默认OFF。使用命令开启一般查询日志： set global general_log=on; log_output表示当查询日志开启以后，以哪种方式存放。FILE表示存放于文件中，TABLE表示存放于表mysql.general_log中，FILE,TABLE表示同时存放于文件和表中, NONE表示不记录日志。log_output不仅控制查询日志，还控制慢查询日志。 general_log_file表示查询日志存放于文件的路径 long_query_time为判定慢查询的时间 慢查询日志 查询超出变量long_query_time指定时间的为慢查询，默认为10秒。但是查询获取锁，包括锁等待的时间不计入查询时间内。慢查询日志是在查询执行完毕且已经完全释放锁之后才记录的，因此慢查询日志记录的顺序和执行的SQL查询语句顺序可能会不一致。例如，语句1先执行，查询速度慢；语句2后执行，但查询速度快，则语句2先记录。 show variables where variable_name like \"slow_query%\" or variable_name = \"log_queries_not_using_indexes\"; +-------------------------------+------------------------------+ | Variable_name | Value | +-------------------------------+------------------------------+ | log_queries_not_using_indexes | OFF | | slow_query_log | ON | | slow_query_log_file | /usr/local/mysql/data/iZbp1j | | | a865ota0u0aap1jmZ-slow.log | +-------------------------------+------------------------------+ slow_query_log表示查询日志是否开启，ON表示开启，OFF表示未开启，默认OFF. 使用命令开启慢查询日志： set global slow_query_log=1; slow_query_log_file表示查询日志存放于文件的路径 log_queries_not_using_indexes表示如果运行的sql语句没有使用到索引，也被记录到慢查询日志，OFF表示不记录，ON表示记录，默认OFF 二进制日志 二进制日志是一个二进制文件，记录了对MySQL数据库执行更改的所有操作，语句以事件的形式记录，所以包括了发生时间、执行时长、操作数据等信息。但是他不记录SELECT、SHOW等那些不改变数据库的SQL语句。二进制日志主要用于数据库恢复和主从复制，以及审计操作。所以出于安全和功能考虑，极不建议将二进制日志和数据目录放在同一磁盘上。 对于事务表的操作，二进制日志只在事务提交的时候一次性写入，提交前的每个二进制日志记录都先cache，提交时写入。所以对于事务表来说，一个事务中可能包含多条二进制日志事件，它们会在提交时一次性写入。而对于非事务表的操作，每次执行完语句就直接写入。 相关变量 show variables where variable_name=\"log_bin\" or variable_name=\"log_bin_basename\" or variable_name=\"max_binlog_size\" or variable_name=\"log_bin_index\" or variable_name=\"binlog_format\" or variable_name=\"sql_log_bin\" or variable_name=\"sync_binlog\"; +------------------+------------+ | Variable_name | Value | +------------------+------------+ | binlog_format | ROW | | log_bin | OFF | | log_bin_basename | | | log_bin_index | | | max_binlog_size | 1073741824 | | sql_log_bin | ON | | sync_binlog | 1 | +------------------+------------+ log_bin表示二进制日志是否开启，ON表示开启，OFF表示未开启，默认OFF log_bin_basename: 二进制日志文件前缀名，二进制日志就记录在该文件中 max_binlog_size: 二进制日志文件的最大大小，超过此大小，二进制文件会自动滚动 log_bin_index: 二进制日志文件索引文件名，用于记录所有的二进制文件 binlog_format：决定了二进制日志的记录方式，STATEMENT以语句的形式记录，ROW以数据修改的形式记录，MIXED以语句和数据修改混合形式记录 sql_log_bin：决定是否对二进制日志进行日志记录，ON表示执行记录，OFF表示不执行记录，默认OFF，这个是会话级别变量可以通SET sql_log_bin = {0|1}来改变该变量值 sync_binlog：决定二进制日志写入磁盘时机，如果sync_binlog为0，操作系统来决定什么时候写入磁盘，如果sync_binlog为N（N=1,2,3..），则每N次事务提交后，都立即将内存中的二进制日志写入磁盘，如何选择取决于安全性与性能的权衡 记录方式 二进制日志的记录方式有3种： STATEMENT：记录对数据库做出修改的语句。比如，update A set test='test'，如果使用statement模式，那么这条update语句将被记录到二进制日志中。使用statement模式，优点是binlog日志量少，IO压力小，性能高，缺点是为了尽可能一致的还原操作，除了记录语句本身外，可能还需要记录一些相关信息，而且，在使用一些特定函数时，并不能保证恢复操作与记录完全一致 ROW：记录对数据库做出的修改的语句所影响到的数据行以及这些行的修改。比如，update A set test='test'，如果使用row模式，那么这条update所影响到的每一行每一列的值都会记录在binlog中。使用row模式时，优点是能完还原和复制被日志记录时的操作，缺点是日志量较大，IO压力比较大，性能消耗比较大 MIXED：混合上述两种模式，一般使用statement方式进行记录，如果遇到一些特殊函数使用row方式进行记录，这种记录方式称为mixed 查看 使用show： SHOW BINARY LOGS # 查看使用了哪些日志文件 SHOW BINLOG EVENTS in 'mysql-bin.000005' from 961; # 查看日志中从位置961开始的日志 SHOW MASTER STATUS # 显式主服务器中的二进制日志信息 恢复数据库 可以使用二进制日志恢复数据库。只需指定二进制日志的起始位置（可指定终止位置）并将其保存到sql文件中，由mysql命令来载入恢复即可。 mysqlbinlog --start-position=373 --stop-position=1871 bin_log_file.000001 > recover_file.sql #临时关闭binlog set sql_log_bin=0; #执行sql文件 source recover_file.sql #开启binlog set sql_log_bin=1; Copyright © 1141134779@qq.com 2020 all right reserved，powered by GitbookFile Modify: 2021-02-04 21:29:31 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"db/redis-types.html":{"url":"db/redis-types.html","title":"Redis数据类型","keywords":"","body":"TreeviewCopyright © 1141134779@qq,com 2020 all right reserved, powered by aleen42 Redis 数据类型 String 应用场景 常用命令 Hash 常用操作 应用场景 常用操作 Set 应用场景 常用操作 Sorted Set 使用场景 常用操作 key通用操作 Redis 数据类型 Redis支持五种数据类型：string（字符串），hash（哈希），list（列表），set（集合）及zset(sorted set：有序集合)。 String string 是 redis 最基本的类型，是key - value存储。string 类型是二进制安全的，可以接受任何数据，一般存放整数、浮点数、字符串，也可以是jpg图片或json。string 类型的值最大能存储 512MB。 应用场景 自增：使用自增（incr）统计网站访问数量，当前在线人数等。redis的操作都是原子性的，不会出现并发冲突。 存储对象：存储 json 或其他对象格式化的字符串。这种场景下推荐使用 hash 数据类型。 set user:id:1 '[{\"id\":1,\"name\":\"aa\",\"email\":\"74326524974@qq.com\"},{\"id\":2,\"name\":\"bb\",\"email\":\"23o978564@qq.com\"}]' 存储MySQL中字段值：把 key 设计为 表名：主键名：主键值：字段名 set user:id:1:email 34876534057@qq.com 常用命令 命令 描述 set key value 设置指定 key 的值 GET key 获取指定 key 的值 INCR key 将 key 中储存的数字值增一 DECR key 将 key 中储存的数字值减一 MGET key1 [key2…] 获取所有(一个或多个)给定 key 的值 SETEX key seconds value 将值 value 关联到 key ，并将 key 的过期时间设为 seconds (以秒为单位) SETNX key value 只有在 key 不存在时设置 key 的值 STRLEN key 返回 key 所储存的字符串值的长度 MSET key value [key value …] 同时设置一个或多个 key-value 对。 GETRANGE key start end 返回 key 中字符串值的子字符 GETSET key value 将给定 key 的值设为 value ，并返回 key 的旧值(old value) Hash hash类型的key是一个唯一值，value是一个hashmap。使用hash类型可以将多个key-value存储到一个key里。 hash天然的适合存储对象和结构化数据。如果使用string存储对象，需要对序列转换和解析，在修改时需要将整个对象反序列化，再修改，然后序列化存储；如果对分别存储对象的多个key-value，容易产生太多的key，浪费内存；使用hash就不没有这些问题。 比如：存储 {{field1: value1},{field2: value2}}: hmset key_name field1 value1 field2 value2 常用操作 命令 描述 HSET key field value 将哈希表 key 中的字段 field 的值设为 value HGET key field 获取存储在哈希表中指定字段的值 HDEL key field2 [field2] 删除一个或多个哈希表字段 HGETALL key 获取在哈希表中指定 key 的所有字段和值 HEXISTS key field 查看哈希表 key 中，指定的字段是否存在 HMGET key field1 [field2] 获取所有给定字段的值 HMSET key field1 value1 [field2 value2 ] 同时将多个 field-value (域-值)对设置到哈希表 key 中 HSETNX key field value 只有在字段 field 不存在时，设置哈希表字段的值 List List 是按照插入顺序排序的字符串链表，可以在头部和尾部插入新的元素。 List使用双向链表实现，两端添加元素的时间复杂度为 O(1)。在头部和尾部插入数据时，性能会非常高，不受链表长度的影响；但如果在链表中插入数据，性能就会越来越差。插入元素时，如果 key 不存在，redis 会为该 key 创建一个新的链表，如果链表中所有的元素都被移除，该 key 也会从 redis 中移除。 应用场景 队列和栈：list的链表实现非常适合队列和栈。可以在其上实现消息队列，生产者将任务push进list，消费者用pop将任务取出。 最新内容：因为 list 结构的数据查询两端附近的数据性能非常好，所以适合一些需要获取最新数据的场景，比如新闻类应用的最近新闻。 常用操作 命令 描述 LPUSH key value1 [value2] 将一个或多个值插入到列表头部 LPOP key 移出并获取列表的第一个元素 LLEN key 获取列表长度 LRANGE key start stop 获取列表指定范围内的元素 LREM key count value 移除列表元素 LTRIM key start stop 对一个列表进行修剪(trim)，就是说，让列表只保留指定区间内的元素，不在指定区间之内的元素都将被删除。 RPUSH key value1 [value2] 在列表中添加一个或多个值 RPOP key 移除并获取列表最后一个元素 RPUSHX key value 为已存在的列表添加值 Set Set 数据类型是一个没有重复元素的无序集合，可以对 set 类型的数据进行添加、删除、判断是否存在等操作，时间复杂度是 O(1) set 集合不允许数据重复，如果添加的数据在 set 中已经存在，将只保留一份。 set 类型提供了多个 set 之间的聚合运算，如求交并补差，这些操作在 redis 内部完成，效率很高。 应用场景 利用交集共同好友列表；或者共同好友多于k时，推荐好友 利用唯一性，统计网站访问量里的独立IP 常用操作 命令 描述 SADD key member1 [member2] 向集合添加一个或多个成员 SCARD key 获取集合的成员数 SDIFF key1 [key2] 返回给定所有集合的差集 SINTER key1 [key2] 返回给定所有集合的交集 SISMEMBER key member 判断 member 元素是否是集合 key 的成员 SMEMBERS key 返回集合中的所有成员 SREM key member1 [member2] 移除集合中一个或多个成员 SUNION key1 [key2] 返回所有给定集合的并集 Sorted Set 有序集合。在 set 的基础上给集合中每个元素关联了一个分数，往有序集合中插入数据时会自动根据这个分数排序。集合中的元素仍然不能重复，但分数可以重复。 sorted set的内部使用HashMap和跳跃表(SkipList)来保证数据的存储和有序，HashMap里放的是成员到score的映射，而跳跃表里存放的是所有的成员，排序依据是HashMap里存的score,使用跳跃表的结构可以获得比较高的查找效率，并且在实现上比较简单。 使用场景 排行榜：积分榜，好友亲密度榜等 带权重的消息队列 常用操作 命令 描述 ZADD key score1 member1 [score2 member2] 向有序集合添加一个或多个成员，或者更新已存在成员的分数 ZCARD key 获取有序集合的成员数 ZCOUNT key min max 计算在有序集合中指定区间分数的成员数 ZINCRBY key increment member 有序集合中对指定成员的分数加上增量 increment ZINTERSTORE destination numkeys key [key …] 计算给定的一个或多个有序集的交集并将结果集存储在新的有序集合 key 中 ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT] 通过分数返回有序集合指定区间内的成员 ZRANK key member 返回有序集合中指定成员的索引 ZREM key member [member …] 移除有序集合中的一个或多个成员 ZREMRANGEBYSCORE key min max 移除有序集合中给定的分数区间的所有成员 ZSCORE key member 返回有序集中，成员的分数值 ZREVRANGEBYSCORE key max min [WITHSCORES] 返回有序集中指定分数区间内的成员，分数从高到低排序 ZUNIONSTORE destination numkeys key [key …] 计算给定的一个或多个有序集的并集，并存储在新的 key 中 key通用操作 命令 描述 DEL key 该命令用于在 key 存在时删除 key。 EXPIRE key seconds 为给定 key 设置过期时间。 EXPIREAT key timestamp EXPIREAT 的作用和 EXPIRE 类似，都用于为 key 设置过期时间。 不同在于 EXPIREAT 命令接受的时间参数是 UNIX 时间戳(unix timestamp)。 EXISTS key 检查给定 key 是否存在 KEYS pattern 查找所有符合给定模式( pattern)的 key MOVE key db 将当前数据库的 key 移动到给定的数据库 db 当中 PERSIST key 移除 key 的过期时间，key 将持久保持 PTTL key 以毫秒为单位返回 key 的剩余的过期时间 TTL key 以秒为单位，返回给定 key 的剩余生存时间(TTL, time to live) TYPE key 返回 key 所储存的值的类型 Copyright © 1141134779@qq.com 2020 all right reserved，powered by GitbookFile Modify: 2021-02-08 16:21:11 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"db/redis-function.html":{"url":"db/redis-function.html","title":"Redis 特性","keywords":"","body":"TreeviewCopyright © 1141134779@qq,com 2020 all right reserved, powered by aleen42 Redis 特性 bitmap 命令 应用 HyperLogLog 特点 命令 应用 GEO 发布订阅 应用 事务 概念 命令 过程 Redis 特性 bitmap bitmap就是用最小的单位bit来代表key，通过设置对应bit的值0或者1，表示某个key对应的值或者状态。redis中bit映射被限制在512MB之内，所以最大是2^32位。bitmap有以下优点： 基于最小的单位bit进行存储，所以非常省空间。 设置时候时间复杂度O(1)、读取时候时间复杂度O(n)，操作是非常快的。 二进制数据的存储，进行相关位运算的时候非常快。 方便扩容 命令 getbit key offset 对key所存储的字符串值，获取指定偏移量上的位（bit） setbit key offset value 对key所存储的字符串值，设置指定偏移量上的位0或1. 返回该位原来的值. offset从0开始，可以超过bitmap的长度。 bitcount key [start end] 获取位图指定范围中位值为1的个数. start和end的单位为byte。如果不指定start与end，则取所有 bitop op destKey key1 [key2...] 做多个BitMap的and、or、not、xor操作, 并将结果保存在destKey中 bitpos key tartgetBit [start end] 计算位图指定范围第一个偏移量对应的的值等于targetBit的位置. 找不到返回-1, start与end没有设置，则取全部 应用 用户在线状态：由于用户ID是唯一的，所以可以用bitmap的每一位代表一个用户ID。在线为1，不在线为0。3亿用户只需要36MB的空间。 用户权限：在应用中的用户权限可能很多，并且经常随着业务修改，比如：是否关闭弹幕，是否接收推送，添加好友是否需要验证等。这些不适合作为固定的属性添加到用户信息表中。可以为每个权限用一个bitmap管理所有用户，这样增加和删除权限时，只需要增加和删除对应的bitmap即可。 HyperLogLog Redis HyperLogLog 是用来做基数统计的数据结构。基数就是不重复元素的数量。比如数据集{1，2，2，2，3，4，5，5}，它的无重复元素的集合是{1，2，3，4，5}，基数就为5. 特点 HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定的、并且是很小的。每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 2^64 个不同元素的基数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。 但是，因为 HyperLogLog 只会根据输入元素来计算基数，而不会储存输入元素本身，所以 HyperLogLog 不能像集合那样，返回输入的各个元素。 命令 PFADD key element [element...] 将元素加入HyperLogLog中。 PFCOUNT key [key...] 返回给定key的基数。多个key会返回基数之和 PFMERGE destkey srckey [srckey...] 将多个HyperLogLog合并成一个。 应用 基数统计可以用在统计网站的独立访客量。比如对商品链接进行访客量统计，需要为每个商品都建立一个数据结构，然后将用户标识存入数据结构，然后统计。 传统的数据结构如B树，可以得到不错的插入和查询代价。但在空间上，如果每个商品的访客量巨大，而商品数量也非常多，那么建立B树将占用巨大的内存，假设每个B树占用1M, 10万个B树就要100G。如果将B树存入磁盘中，很难进行合并操作。如果使用bitmap作为集合，bitmap可以高效的查询和合并。但bitmap的内存占用与基数的上限有关，假如要计算上限为1千万的基数，则需要1M字节的bitmap，10万个商品同样要100G，并且每个bitmap的空间是固定分配的。所以bitmap也不合适。采用HyperLogLog则可以很好的解决需求。 GEO Redis GEO 主要用于存储地理位置信息，并且可以进行相关计算。 https://www.runoob.com/redis/redis-geo.html 发布订阅 Redis 发布订阅 (pub/sub) 是一种消息通信模式：发布者发送消息，订阅者接收消息。 当一个客户端通过 PUBLISH 命令向订阅者发送信息的时候，我们称这个客户端为发布者(publisher)。 而当一个客户端使用 SUBSCRIBE 或者 PSUBSCRIBE命令接收信息的时候，我们称这个客户端为订阅者(subscriber)。一个客户端可以订阅任意数量的channel。 为了解耦发布者和订阅者之间的关系，Redis 使用了 channel 作为两者的中介—— 发布者将信息直接发布给 channel ，而 channel 负责将信息发送给适当的订阅者。发布者和订阅者之间没有相互关系，也不知道对方的存在。 应用 任务通知：系统向channel中发布任务，用户作为订阅者从channel中获取任务。不同的用户群体通过不同的channel获得各自的任务。 参数刷新：当前端的参数需要更新时，比如轮播图、广告等，可以使用发布订阅来通知各个前端。 事务 概念 Redis 事务的本质是一组命令的集合。事务支持一次性执行多个命令，一个事务中所有命令都会被序列化。在事务执行过程，会按照顺序执行队列中的命令。其他客户端提交的命令请求不会插入到事务的命令序列中。 Redis事务没有隔离级别的概念：批量操作在发送 EXEC 命令前被放入队列缓存，并不会被实际执行，也就不存在事务内的查询要看到事务里的更新，事务外查询不能看到。 Redis不保证原子性：单条命令是原子性执行的，但事务不保证原子性，且没有回滚。事务中任意命令执行失败，其余的命令仍会被执行。 Redis中的事务并没有关系型数据库中的事务回滚(rollback)功能。使用者必须自行处理，不过这也使得Redis的事务简洁快速。 命令 MULTI: 标记一个事务的开始。在这下面输入的命令都会被加入事务队列。 EXEC：执行事务队列的命令。 DISCARD：取消事务，放弃队列里的所有命令。 WATCH：事务中的命令要全部执行完之后才能获取每个命令的结果，但是如果一个事务中的命令B依赖于他上一个命令A的结果，就需要使用WATCH来监视key。如果在事务执行之前，被监视的key被其他命令改动，则事务被打断 （ 类似乐观锁 ） 当事务执行EXEC后，无论事务使用执行成功， WATCH 对变量的监控都将被取消。所以当事务执行失败后，需重新执行WATCH命令对变量进行监控，并开启新的事务进行操作。 UNWATCH：取消 WATCH 命令对所有 key 的监视。 过程 开始事务 MULTI 命令入队：输入的命令会加入队列中，不会立即执行。 set k1 v1 set k2 v2 ... Redis会检查语法错误，如果其中的命令有错误，事务中的所有命令都不被执行。 执行事务 EXEC 如果在命令运行过程中出现运行错误，比如用GET获取一个hash类型的键值，这种错误在命令执行之前是无法发现的，事务中这样的命令仍然会被执行，其他的命令也会被执行。 如果WATCH的值被修改，事务将会被打断然后取消。 Copyright © 1141134779@qq.com 2020 all right reserved，powered by GitbookFile Modify: 2021-02-06 21:29:25 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"db/redis-single-thread.html":{"url":"db/redis-single-thread.html","title":"Redis 为什么快","keywords":"","body":"TreeviewCopyright © 1141134779@qq,com 2020 all right reserved, powered by aleen42 Redis 为什么快 性能 原因 单线程 I/O多路复用 Redis 为什么快 性能 Redis采用的是基于内存的是单进程单线程模型的 KV 数据库，官方提供的数据是可以达到100000+的QPS。有兴趣的可以参考官方的基准程序测试《How fast is Redis？》 横轴是连接数，纵轴是QPS。可以看出Redis的性能十分强大。但是它为什么这么快呢？ 原因 基于内存。绝大部分请求是纯粹的内存操作，非常快速。 数据结构简单。Redis中的数据结构是专门进行设计的，一些常用操作的时间复杂度都是O(1)。 单线程。不需要考虑锁的问题，节省了大量的加锁和阻塞的时间，避免了因死锁而导致的性能消耗。 使用多路复用I/O模型。 自己构建的底层模型。Redis自己构建了VM 机制 ，因为调用系统函数，会浪费一定的时间去移动和请求； 单线程 与多线程相比，Redis单线程的工作方式可以减少线程调度的耗时，在数据上，也不需要考虑锁，从而节省了加锁和等待锁的时间，也避免了出现死锁造成的性能消耗。 多线程真的比单线程快吗？多线程的本质是，通过CPU调度来模拟多个线程。用一个CPU做同样的内存操作，用多线程的方式会比单线程多出上下文切换的时间。对于内存系统，没有上下文切换是效率最高的。redis 用 单个CPU 绑定一块内存，然后针对这块内存的数据在一个CPU上完成读写，这时采用单线程是最佳方案。 Redis is single threaded. How can I exploit multiple CPU / cores? It's not very frequent that CPU becomes your bottleneck with Redis, as usually Redis is either memory or network bound. For instance, using pipelining Redis running on an average Linux system can deliver even 1 million requests per second, so if your application mainly uses O(N) or O(log(N)) commands, it is hardly going to use too much CPU. However, to maximize CPU usage you can start multiple instances of Redis in the same box and treat them as different servers. At some point a single box may not be enough anyway, so if you want to use multiple CPUs you can start thinking of some way to shard earlier. You can find more information about using multiple Redis instances in the Partitioning page. However with Redis 4.0 we started to make Redis more threaded. For now this is limited to deleting objects in the background, and to blocking commands implemented via Redis modules. For future releases, the plan is to make Redis more and more threaded. 官方表示，CPU不是Redis的瓶颈，Redis的瓶颈最有可能是机器内存的大小或者网络带宽。 需要注意的是，这里的单线程仅仅是处理网络请求，完整的redis是不止一个线程的。比如，redis做持久化的时候另外开辟线程执行。官方也说到，之后版本会用多线程完成一些别的操作。 那么，什么场景适合多线程呢？ 磁盘这种慢速的操作。频繁的磁盘读写会比内存多上数量级的耗时。这时可以利用磁盘的吞吐量，用一个线程管理任务队列，达到一定数量时，统一进行磁盘读取和写入。然后多个请求线程只需将请求加入队列，然后处理线程异步的返回数据就好了。这样可以尽量的利用磁盘的性能，在磁盘面前多线程调度的耗时可以忽略。 如何利用多核性能？ 可以在单机开多个Redis实例，因为k-v存储的数据之间没有约束，只需要分清数据在哪个实例上就好。 I/O多路复用 多路指的是多个网络连接，复用指的是复用同一个线程。多路复用模型是利用 epoll 同时监察多个流的 I/O 事件。在空闲的时候，会把当前线程阻塞掉，当有一个或多个流有 I/O 事件时，线程从阻塞态中唤醒，程序就会轮询发生事件的流，并且只依次顺序的处理就绪的流，这种做法就避免了大量的无用操作。 再加上 Redis 本身的事件处理模型将 epoll 中的连接、读写、关闭都转换为事件，放入队列中排队，事件分派器每次从队列中取出一个事件，把该事件交给对应的事件处理器进行处理，不在网络 IO 上浪费过多的时间。 Copyright © 1141134779@qq.com 2020 all right reserved，powered by GitbookFile Modify: 2021-02-07 21:09:57 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"db/redis-presistence.html":{"url":"db/redis-presistence.html","title":"Redis 持久化","keywords":"","body":"TreeviewCopyright © 1141134779@qq,com 2020 all right reserved, powered by aleen42 Redis 持久化 前言 RDB 手动备份 BGSAVE 自动备份 优点 缺点 AOF 过程 触发重写 优点 缺点 RDB和AOF的选择 Redis 持久化 前言 持久化就是将内存中的数据保存到硬盘上，当发生故障时，数据可以从硬盘上恢复。持久化机制也可以用于拓展节点时完成主从同步。 Redis 提供了两种持久化机制：RDB快照，和AOF日志。 RDB RDB持久化是默认的持久化方式，在指定的时间间隔内将内存中的数据集快照写入磁盘。在进行数据持久化的过程中，会先将数据写入到一个临时文件中，待持久化过程都结束了，才会用这个临时文件替换上次持久化好的文件。正是这种特性，让我们可以随时来进行备份，因为快照文件总是完整可用的。 手动备份 BGSAVE 执行BGSAVE命令，会进行以下操作： ，Redis父进程判断当前是否存在正在执行的子进程，如RDB/AOF子进程，如果存在，bgsave命令直接返回。 父进程 fork 出一个子进程。fork的短暂时间内，父进程会阻塞； 直到fork完成，bgsave命令返回“Background saving started”信息，并不再阻塞父进程，可以继续响应其他命令。此时父进程和子进程是共享内存的，如果父进程又得到了一个写命令，copy-on-write机制会复制出一个页副本来处理新的请求。 子进程创建RDB文件，根据父进程内存生成临时快照文件，完成后对原有文件进行原子替换。 进程发送信号给父进程表示完成，父进程更新统计信息。 自动备份 可以通过配置设置自动做快照持久化的方式。我们可以配置redis在n秒内如果超过m个key被修改就自动执行BGSAVE，下面是默认的快照保存配置。 save 900 1 #900秒内如果超过1个key被修改，则发起快照保存 save 300 10 #300秒内容如超过10个key被修改，则发起快照保存 save 60 10000 如果从节点执行全量复制操作，主节点自动执行bgsave生成RDB文件并发送给从节点。执行shutdown命令时，如果没有开启AOF持久化功能，也会自动执行bgsave。 优点 RDB是一个紧凑压缩的二进制文件，代表Redis在某个时间点上的数据快照。非常适用于备份，容灾等场景。 Redis加载RDB恢复大型数据集远远快于AOF的方式。 redis会单独创建一个子进程来进行持久化，而主进程是不会进行任何IO操作的，这样就确保了redis极高的性能。 缺点 RDB要保存整个数据集的文件，它并不是一个轻松的操作。可能至少5分钟后才会保存一次RDB文件。如果在一次RDB持久化之后的几分钟里发生宕机，那么这几分钟内写入的数据都将丢失。 AOF AOF，即Append Only File，只允许追加不允许改写的文件。AOF方式是将执行过的写指令记录下来，在数据恢复时按照从前到后的顺序再将指令都执行一遍。AOF解决了数据持久化的实时性，目前已经是Redis持久化的主流方式。恢复数据时，如果AOF持久化开启且存在AOF文件时，优先加载AOF文件。 默认的AOF持久化策略是每秒钟fsync一次（fsync是指把缓存中的写指令记录到磁盘中），因为在这种情况下，redis仍然可以保持很好的处理性能，即使redis故障，也只会丢失最近1秒钟的数据。 过程 所有的写命令都追加到AOF缓冲区中。Redis使用单线程响应命令，如果每次写AOF文件命令都直接追加到硬盘，那么性能完全取决于当前硬盘负载。选择先写入缓冲区中，Redis可以提供多种缓冲区同步硬盘的策略，在性能和安全性方面做出平衡。 AOF缓冲区根据对应的策略向硬盘做同步操作。 随着AOF文件越来越大，需要定期对AOF文件进行重写，达到压缩的目的。重写的工作包括： 清除已经超时的数据的写入命令； 重写使用内存中的数据直接生成，这样新的AOF文件只保留最终数据的写入命令； 将多条写命令合并成一个。 重写时，父进程先fork出子进程，子进程对旧的AOF文件重写，生成新的AOF文件 同时父进程仍然在响应其他的命令。如果在子进程重写的同时，父进程仍然有写命令，就将写命令加入AOF重写缓冲区中。 子进程重写完旧的AOF文件后，将AOF重写缓冲区里的命令写入新的AOF文件。 用新的AOF文件替换旧的。 触发重写 手动触发：直接调用bgrewriteaof命令。 自动触发：设置以下参数，超出后重写 auto-aof-rewrite-min-size： AOF文件大小阈值， auto-aof-rewrite-percentage：当前AOF文件大小和上一次重写后AOF文件大小的比值 appendfsync 每秒同步 优点 AOF可以更好的保护数据不丢失，一般AOF会每隔1秒，通过一个后台线程执行一次持久化操作，redis仍然可以保持很好的处理性能，即使redis故障，也只会丢失最近1秒钟的数据。 在进行AOF重写时，仍然是采用先写临时文件，全部完成后再替换的流程，所以断电、磁盘满等问题都不会影响AOF文件的可用性。 AOF 文件有序地保存了对数据库执行的所有写入操作，文件的内容非常容易被人读懂， 对文件进行分析也很轻松。 缺点 对于同一份数据来说，AOF日志文件通常比RDB数据快照文件更大。 AOF开启后，支持的写QPS会比RDB支持的写QPS低，因为AOF一般会配置成每秒fsync一次日志文件，当然，每秒一次时的性能也还是很高的 恢复速度不如RDB快。 RDB和AOF的选择 如果希望获得最佳的性能，并且可以承受数分钟以内的数据丢失，那么可以只使用 RDB 持久化。 如果想得到更可靠的持久化方案， 你应该同时使用两种持久化功能。恢复时第一时间用RDB，然后用AOF做数据补全。 Copyright © 1141134779@qq.com 2020 all right reserved，powered by GitbookFile Modify: 2021-02-07 23:18:26 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"linux/chmod.html":{"url":"linux/chmod.html","title":".sh 添加执行权限","keywords":"","body":"给 .sh 文件添加执行权限 chmod u+x file.sh chmod (change the permissions mode of a file) 权限管理命令。 u代表所有者，x代表执行权限。 + 表示增加权限。 Copyright © 1141134779@qq.com 2020 all right reserved，powered by GitbookFile Modify: 2021-01-08 13:47:16 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"other/reverseproxy.html":{"url":"other/reverseproxy.html","title":"Docker 安装 Nginx 并实现反向代理","keywords":"","body":"TreeviewCopyright © 1141134779@qq,com 2020 all right reserved, powered by aleen42 Docker 安装 Nginx 并实现反向代理 安装 Nginx 镜像 拉取镜像 创建实例 创建关键目录映射点 反向代理 Docker 安装 Nginx 并实现反向代理 安装 Nginx 镜像 拉取镜像 docker pull nginx 查看镜像 docker images 创建实例 docker run --name nginx-test -p 80:80 -d nginx docker start nginx-test 浏览器访问服务器ip，运行nginx欢迎页面 创建关键目录映射点 本地创建文件夹 html: nginx存储网页的目录 logs：日志目录 conf：配置文件目录 mkdir -p nginx/html nginx/logs nginx/conf 将刚才创建的nginx容器的配置文件拷贝到本地 docker cp nginx-test:/etc/nginx ~/nginx/conf 创建新的nginx容器 docker run -d -p 80:80 --name nginx -v ~/nginx/html:/usr/share/nginx/html -v ~/nginx/logs:/var/log/nginx -v ~/nginx/conf:/etc/nginx nginx docker start nginx 反向代理 反向代理就是将访问本机的数据代理转发到本机的其他端口。比如，一个网站在4000端口上，而浏览器默认访问而的是80端口。就可以用nginx将访问80端口的数据转发到4000端口上，然后浏览器直接访问80端口就可以看到网站了。 所以要先知道要反向代理的服务的地址。由于我们要代理的是容器，先查看容器的ip docker inspect 容器名 在最后的Networks下看到IPAddress 编辑conf/conf.d/default.conf，修改proxy_pass 为目标容器的ip和端口号 location / { #root /usr/share/nginx/html; #index index.html index.htm; proxy_pass http://172.17.0.2:4000; } 访问浏览器，成功跳转到目标网页 Copyright © 1141134779@qq.com 2020 all right reserved，powered by GitbookFile Modify: 2021-01-22 22:23:54 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"other/git.html":{"url":"other/git.html","title":"Git同步本地项目到Github","keywords":"","body":"TreeviewCopyright © 1141134779@qq,com 2020 all right reserved, powered by aleen42 Git同步本地项目到Github 本地git获取github提交权限 设置用户名和邮箱 生成ssh密钥 复制到github 测试完成 创建本地仓库 创建远程仓库 将本地仓库同步到远程仓库 添加远程仓库 将本地仓库的内容push到远程仓库的master分支 Git同步本地项目到Github 将本地的项目同步到github上，这样可以随时pull到本地，修改完后再push到github仓库。可以随时随地修改代码，也避免了项目的丢失的风险。 本地git获取github提交权限 设置用户名和邮箱 git config --global user.name 'your_name' git config --global user.email 'your_email' 生成ssh密钥 ssh-keygen -t rsa -C 'your_email' 提示设置存储位置和口令等，回车跳过。默认存储在 ~/.ssh/id_rsa.pub 复制到github 将生成的id_rsa.pub文件中的公钥复制到github的setting / SSH AND GPG KEY / SSH keys 测试完成 ssh git@github.com 提示 successfully authenticated 则成功。 创建本地仓库 cd到项目目录 git init 初始化git仓库 git add . 把所有项目文件添加到提交暂存区 git commit -m '提交说明' 把暂存区中的内容提交到仓库 创建远程仓库 github新建仓库，假设仓库名为[resName] 将本地仓库同步到远程仓库 添加远程仓库 git remote add origin git@github.com:[githubUerName]/[resName] 将本地仓库的内容push到远程仓库的master分支 git push -u origin master push的-u参数是设置本地仓库默认的upstream,这里就是把本地仓库同远程仓库的master分支进行关联，之后你在这个仓库pull时不带参数也默认从master分支拉取. Copyright © 1141134779@qq.com 2020 all right reserved，powered by GitbookFile Modify: 2021-01-07 21:37:30 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"other/gitbook.html":{"url":"other/gitbook.html","title":"使用 Gitbook 搭建博客","keywords":"","body":"TreeviewCopyright © 1141134779@qq,com 2020 all right reserved, powered by aleen42 使用 Gitbook 搭建博客 安装 node.js 编辑工具 Gitbook 初始化 目录 启动服务 插件 webhook 实现服务器自动更新博客 docker 部署 webhook 启动 webhook 配置 github 使用 Gitbook 搭建博客 gitbook 使用markdown 编写，简单易用。通过配置插件，也可以添加很多主题和小功能。适合搭建博客，电子书等。 安装 node.js gitbook是一个基于Node.js的命令行工具，所以要先安装Node.js(下载地址https://nodejs.org/en/，找到对应平台的版本安装即可)。 或使用包管理安装。 apt-get install nodejs 安装Gitbook： npm install -g gitbook-cli 编辑工具 typora：https://www.typora.io/ apt-get install typora Gitbook 初始化 在空文件夹中执行 gitbook init 文件夹中将多两个文件 README.md ：封面介绍 SUMMARY.md ：配置目录结构 目录 编辑SUMMARY.md # Summary * [Introduction](README.md) * [前言](readme.md) * [第一章](part1/README.md) * [第一节](part1/1.md) * [第二节](part1/2.md) * [第三节](part1/3.md) * [第四节](part1/4.md) * [第二章](part2/README.md) * [第三章](part3/README.md) * [第四章](part4/README.md) 编辑后执行 gitbook init 将自动按以上目录寻找或创建文件。 每一篇文章都是一个.md文件，这样就可以开始写博客了。 启动服务 编辑好文件后，执行 gitbook init gitbook serve gitbook将在本地4000端口启动服务。浏览器访问 http://localhost:4000/ 至此，已经可以使用 gitbook 搭建博客了。 插件 文件夹下创建 book.json ，如果已有，就直接打开。 配置插件等都是在这里，也可以复制别人的配置项。 plugins 是配置插件的位置，gitbook自带了5个插件，在名字前面加 - 可以禁用插件： sharing：右上角分享功能 font-settings：字体设置（左上方的\"A\"符号） livereload：为 GitBook 实时重新加载 highlight： 代码高亮 search： 导航栏查询功能（不支持中文） 推荐几个我在使用的插件： page-treeview：每篇文档头部生成标题树 code：为代码块添加行号和复制按钮 pageview-count：阅读量计数 popup：插件用于点击图片时，打开新的网页用来查看高清大图。 tbfed-pagefooter：在每个页面的最下方自动添加页脚信息 favicon：修改网页标题的图标 search-plus：原搜索插件不支持中文搜索，所以使用该插件进行替换。 expandable-chapters 和 chapter-fold ：导航目录 hide-element：隐藏界面元素 back-to-top-button：返回顶部按钮 splitter：侧边栏可自行调整宽度 sharing-plus：分享当前页面，比默认的 sharing 插件多了一些分享方式。 donate：打赏模块，在每篇文章底部都会加上一个按钮，点击显示图片 github：右上角跳转到 github 主页 附上我的配置文件 { \"author\": \"Bourbon\", \"description\": \"学习，记录，分享，进步\", \"extension\": null, \"generator\": \"site\", \"isbn\": \"\", \"links\": { \"sharing\": { \"all\": null, \"facebook\": null, \"google\": null, \"twitter\": null, \"weibo\": null } }, \"output\": null, \"pdf\": { \"fontSize\": 12, \"footerTemplate\": null, \"headerTemplate\": null, \"margin\": { \"bottom\": 36, \"left\": 62, \"right\": 62, \"top\": 36 }, \"pageNumbers\": true, \"paperSize\": \"a4\" }, \"plugins\": [ \"page-treeview\", \"code\", \"pageview-count\", \"popup\", \"tbfed-pagefooter\", \"favicon\", \"search-plus\", \"expandable-chapters\", \"hide-element\", \"back-to-top-button\", \"splitter\", \"-lunr\", \"-search\", \"-sharing\", \"sharing-plus\", \"chapter-fold\", \"donate\", \"github\" ], \"pluginsConfig\": { \"code\": { \"copyButtons\": false }, \"hide-element\": { \"elements\": [\".gitbook-link\"] }, \"tbfed-pagefooter\": { \"copyright\": \"Copyright © 1141134779@qq.com 2020\" }, \"favicon\": { \"shortcut\": \"assert/favicon.ico\", \"bookmark\": \"assert/favicon.ico\", \"appleTouch\": \"assert/favicon.ico\", \"appleTouchMore\": { \"120x120\": \"assert/favicon.ico\", \"180x180\": \"assert/favicon.ico\" } }, \"fontsettings\": { \"theme\": \"white\", \"family\": \"sans\", \"size\": 2 }, \"page-treeview\": { \"copyright\": \"Copyright © 1141134779@qq,com 2020\", \"minHeaderCount\": \"2\", \"minHeaderDeep\": \"2\" }, \"sharing\": { \"all\": [\"facebook\", \"google\", \"linkedin\", \"twitter\", \"weibo\", \"qq\"] }, \"donate\": { \"wechat\": \"/assert/wechat.jpg\", \"alipay\": \"/assert/alipay.jpg\", \"title\": \"\", \"button\": \"赏\", \"alipayText\": \"支付宝打赏\", \"wechatText\": \"微信打赏\" }, \"github\": { \"url\": \"https://github.com/BourbonWang\" } }, \"language\": \"zh-hans\", \"title\": \"Bourbon\", \"variables\": {}, \"styles\": { \"website\": \"/assert/styles/website.css\" } } webhook 实现服务器自动更新博客 由于需要频繁更新博客，不可能每次更新都重新上传服务器，所以需要服务器自动拉取gitbook文件夹。可以将gitbook上传到github，然后使用 webhook 将push与服务器关联。这样每次更新后push到github，然后webhook执行服务器的脚本，将文件夹 pull 下来，重启gitbook 服务。 docker 这里将博客放到docker里，方便管理，不会与其他服务冲突。 拉取 node 镜像，创建容器 docker run -itd --name blog -p 4000:4000 node:10.19 /bin/bash docker exec -it blog /bin/bash 容器内同样方法安装gitbook。 部署 webhook apt 换源(debian)后，安装 pip apt-get install python3-pip 我使用的 webhookit：https://github.com/hustcc/webhookit 可根据文档自行安装。 由于这个webhook只能用python2, 注意使用 pip2。 pip2 install webhookit webhookit_config > /home/webhook/config.py 编辑config.py ，只需要修改repo_name/branch_name 和 SCRIPT # -*- coding: utf-8 -*- ''' Created on Mar-03-17 15:14:34 @author: hustcc/webhookit ''' # This means: # When get a webhook request from `repo_name` on branch `branch_name`, # will exec SCRIPT on servers config in the array. WEBHOOKIT_CONFIGURE = { # a web hook request can trigger multiple servers. 'repo_name/branch_name': [{ # if exec shell on local server, keep empty. 'HOST': '', # will exec shell on which server. 'PORT': '', # ssh port, default is 22. 'USER': '', # linux user name 'PWD': '', # user password or private key. # The webhook shell script path. 'SCRIPT': '/home/webhook/shell.sh' }, ...], ... } 创建shell.sh，就是自动执行的脚本，用来拉取博客，完成更新 cd /home/blog git pull gitbook install gitbook init gitbook serve 启动 webhook webhookit -c /home/webhook/config.py -p 4001 监听4001端口，浏览器访问即可查看webhook URL以及配置信息。 配置 github 仓库 -> Settings -> Webhooks -> Add webhook payload URL：填写webhook URL Content type ：application/json 触发条件：Just the push event. Copyright © 1141134779@qq.com 2020 all right reserved，powered by GitbookFile Modify: 2021-02-03 22:37:32 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"}}